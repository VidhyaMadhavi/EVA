{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlUqHTsGFdRg",
        "colab_type": "text"
      },
      "source": [
        "# Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRGeWZssQD2-",
        "colab_type": "text"
      },
      "source": [
        "-  Copy the file to your collaboratory\n",
        "-  You need to:\n",
        "-  write comments for all the cells\n",
        "-  define a new network such that:\n",
        "-  it has less than 20000 parameters\n",
        "-  it achieves validation accuracy of more than 99.4% (basically print(score) should be more than 0.994\n",
        "-  Once done, upload the link to your Github Project to LMS. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxWdvpvTFgcE",
        "colab_type": "text"
      },
      "source": [
        "### Mount the Drive on Collab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR8CatOWVOnt",
        "colab_type": "code",
        "outputId": "4ddcf6ae-8c06-451b-c315-31ad368de301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/EVA3/EVA3-Assignment3')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfOxD6_WFlUj",
        "colab_type": "text"
      },
      "source": [
        "## Installation of Necessary Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IypwJFph7i7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "# !pip install -q keras\n",
        "# import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUS9S-TjFpJk",
        "colab_type": "text"
      },
      "source": [
        "## Import ie load the necessary packages in to Memory so that we can use them in the program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcJil51eD0WT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Convolution2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ak121beFwB7",
        "colab_type": "text"
      },
      "source": [
        "## Load the Mnist Data  in to Training Set and Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0ge8yO-D3wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuXOf3jyF2_j",
        "colab_type": "text"
      },
      "source": [
        "## Display one row one datapoint from the Training Set. Each point is a handwritten Digit\n",
        "\n",
        "\n",
        "# Size of Train Dataset and plot the first number on the graph\n",
        "- The train dataset has input data which is stored in X_train\n",
        "- X_train is an 3 dimensinal array which is storing 60 thousand numbers. Each number is of 28 x 28 of size i.e 28 rows and 28 columns\n",
        "\n",
        "### Now plot the first number\n",
        "- Among the 60000 thousand number fetch the frist number by calling X_train[0]\n",
        "- the first number has 28 rows and 28 columns.\n",
        "- plot the first number matplotlib. this is the library used to plot graphs\n",
        "- plt.imshow function helps to show the graph on the screen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouZ2UxI_EHju",
        "colab_type": "code",
        "outputId": "9750c58f-926d-48f5-b3f8-70aff5c1e8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe6a554f2e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln2kIxVaGR0W",
        "colab_type": "text"
      },
      "source": [
        "## ## Reshape the 3 dimensional array to 4 dimensional Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RUeS5BYEWQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmdlPriVGTks",
        "colab_type": "text"
      },
      "source": [
        "## Conver the Training data in to float and then normalize the data by dividing each point by 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-9ksR4REeq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zIlmvi1OQwV",
        "colab_type": "code",
        "outputId": "46d27234-074f-4ebf-a23d-32511f853d14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-JTy_4HOSqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_dLt3BcOUvv",
        "colab_type": "code",
        "outputId": "bbc9f005-65d8-4733-8ce1-d0f626c28a59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi8tKqNBGoq_",
        "colab_type": "text"
      },
      "source": [
        "## Model Approach Training Till the Training Accuracy is more than 99.8 and then perform the testing onthe Test Data\n",
        "## Parameters are 14,736\n",
        "\n",
        "\n",
        "\n",
        "- ## Compiling the model \n",
        "\n",
        "- mentioning which Optimizer algorithm to be used to calculate the gradients\n",
        "\n",
        "## Train the Model\n",
        "\n",
        "- Pass the input data X_train\n",
        "- Pass the output data y_Train\n",
        "- Batch_size - 32. Each epoch is to cover all images one time. To do that the images goes in batches. For one epoch there will be multiple batches. Each batch will take 32 images.\n",
        "\n",
        "- nb_epoch = 10. Train the model for 1000 epochs, stop it when you reach required accuracy\n",
        "\n",
        "- Verbose - show the training log history for every epoch.\n",
        "\n",
        "\n",
        "\n",
        "# Validate/ Test the Model\n",
        "\n",
        "- After the model is trained we test the model by checking the predictions with y_test values. \n",
        "- Accuracy score is calculates by checking how many predictions are correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXNK1gFCPBqK",
        "colab_type": "text"
      },
      "source": [
        "keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "\n",
        "\n",
        " - #### padding='valid'  : No \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTdPSgwBOYFy",
        "colab_type": "code",
        "outputId": "db213033-81d0-4149-c829-c80ab06e5ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation, Conv2D, MaxPooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Conv2D(16, (3, 3) , activation='relu',use_bias=False, input_shape=(28,28,1)))  #26\n",
        "\n",
        "model.add(Conv2D(16, (3, 3) , activation='relu',use_bias=False))     # 24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(32, (3,3), activation = 'relu', use_bias= False))   # 22\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))      # 11, 11 \n",
        "\n",
        "model.add(Conv2D(16, (3,3), activation = 'relu', use_bias= False))   # 22\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(16, (3,3), activation = 'relu', use_bias= False))   # 22\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(10, (2,2), activation='relu',use_bias=False))          # 12x12, 10 layers\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1000, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_114 (Conv2D)          (None, 26, 26, 16)        144       \n",
            "_________________________________________________________________\n",
            "conv2d_115 (Conv2D)          (None, 24, 24, 16)        2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_116 (Conv2D)          (None, 22, 22, 32)        4608      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_117 (Conv2D)          (None, 9, 9, 16)          4608      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_118 (Conv2D)          (None, 2, 2, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 2, 2, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 2, 2, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_119 (Conv2D)          (None, 1, 1, 10)          640       \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,736\n",
            "Trainable params: 14,672\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "60000/60000 [==============================] - 26s 433us/step - loss: 0.2441 - acc: 0.9343\n",
            "Epoch 2/1000\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0727 - acc: 0.9772\n",
            "Epoch 3/1000\n",
            "60000/60000 [==============================] - 25s 420us/step - loss: 0.0584 - acc: 0.9821\n",
            "Epoch 4/1000\n",
            "60000/60000 [==============================] - 25s 419us/step - loss: 0.0498 - acc: 0.9849\n",
            "Epoch 5/1000\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0439 - acc: 0.9861\n",
            "Epoch 6/1000\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0395 - acc: 0.9873\n",
            "Epoch 7/1000\n",
            "60000/60000 [==============================] - 25s 422us/step - loss: 0.0362 - acc: 0.9885\n",
            "Epoch 8/1000\n",
            "60000/60000 [==============================] - 25s 412us/step - loss: 0.0327 - acc: 0.9896\n",
            "Epoch 9/1000\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0306 - acc: 0.9906\n",
            "Epoch 10/1000\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0291 - acc: 0.9912\n",
            "Epoch 11/1000\n",
            "60000/60000 [==============================] - 26s 432us/step - loss: 0.0281 - acc: 0.9914\n",
            "Epoch 12/1000\n",
            "60000/60000 [==============================] - 25s 417us/step - loss: 0.0260 - acc: 0.9917\n",
            "Epoch 13/1000\n",
            "60000/60000 [==============================] - 25s 424us/step - loss: 0.0254 - acc: 0.9915\n",
            "Epoch 14/1000\n",
            "60000/60000 [==============================] - 25s 418us/step - loss: 0.0240 - acc: 0.9924\n",
            "Epoch 15/1000\n",
            "60000/60000 [==============================] - 25s 418us/step - loss: 0.0215 - acc: 0.9929\n",
            "Epoch 16/1000\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0223 - acc: 0.9930\n",
            "Epoch 17/1000\n",
            "60000/60000 [==============================] - 25s 416us/step - loss: 0.0201 - acc: 0.9939\n",
            "Epoch 18/1000\n",
            "60000/60000 [==============================] - 25s 420us/step - loss: 0.0191 - acc: 0.9938\n",
            "Epoch 19/1000\n",
            "60000/60000 [==============================] - 26s 427us/step - loss: 0.0196 - acc: 0.9937\n",
            "Epoch 20/1000\n",
            "60000/60000 [==============================] - 25s 425us/step - loss: 0.0174 - acc: 0.9938\n",
            "Epoch 21/1000\n",
            "60000/60000 [==============================] - 26s 433us/step - loss: 0.0175 - acc: 0.9944\n",
            "Epoch 22/1000\n",
            "60000/60000 [==============================] - 25s 411us/step - loss: 0.0173 - acc: 0.9945\n",
            "Epoch 23/1000\n",
            "60000/60000 [==============================] - 25s 418us/step - loss: 0.0155 - acc: 0.9947\n",
            "Epoch 24/1000\n",
            "60000/60000 [==============================] - 25s 410us/step - loss: 0.0162 - acc: 0.9943\n",
            "Epoch 25/1000\n",
            "60000/60000 [==============================] - 24s 394us/step - loss: 0.0155 - acc: 0.9949\n",
            "Epoch 26/1000\n",
            "60000/60000 [==============================] - 24s 406us/step - loss: 0.0150 - acc: 0.9951\n",
            "Epoch 27/1000\n",
            "60000/60000 [==============================] - 25s 421us/step - loss: 0.0153 - acc: 0.9949\n",
            "Epoch 28/1000\n",
            "60000/60000 [==============================] - 24s 401us/step - loss: 0.0140 - acc: 0.9957\n",
            "Epoch 29/1000\n",
            "60000/60000 [==============================] - 24s 402us/step - loss: 0.0133 - acc: 0.9954\n",
            "Epoch 30/1000\n",
            "60000/60000 [==============================] - 24s 408us/step - loss: 0.0144 - acc: 0.9951\n",
            "Epoch 31/1000\n",
            "60000/60000 [==============================] - 24s 399us/step - loss: 0.0129 - acc: 0.9957\n",
            "Epoch 32/1000\n",
            "60000/60000 [==============================] - 25s 421us/step - loss: 0.0128 - acc: 0.9957\n",
            "Epoch 33/1000\n",
            "60000/60000 [==============================] - 25s 412us/step - loss: 0.0131 - acc: 0.9958\n",
            "Epoch 34/1000\n",
            "60000/60000 [==============================] - 25s 417us/step - loss: 0.0118 - acc: 0.9961\n",
            "Epoch 35/1000\n",
            "60000/60000 [==============================] - 24s 404us/step - loss: 0.0113 - acc: 0.9960\n",
            "Epoch 36/1000\n",
            "60000/60000 [==============================] - 25s 410us/step - loss: 0.0123 - acc: 0.9957\n",
            "Epoch 37/1000\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0109 - acc: 0.9960\n",
            "Epoch 38/1000\n",
            "60000/60000 [==============================] - 24s 400us/step - loss: 0.0107 - acc: 0.9962\n",
            "Epoch 39/1000\n",
            "60000/60000 [==============================] - 24s 406us/step - loss: 0.0125 - acc: 0.9957\n",
            "Epoch 40/1000\n",
            "60000/60000 [==============================] - 25s 419us/step - loss: 0.0123 - acc: 0.9958\n",
            "Epoch 41/1000\n",
            "60000/60000 [==============================] - 25s 424us/step - loss: 0.0093 - acc: 0.9967\n",
            "Epoch 42/1000\n",
            "60000/60000 [==============================] - 24s 406us/step - loss: 0.0106 - acc: 0.9963\n",
            "Epoch 43/1000\n",
            "60000/60000 [==============================] - 24s 405us/step - loss: 0.0105 - acc: 0.9964\n",
            "Epoch 44/1000\n",
            "60000/60000 [==============================] - 26s 427us/step - loss: 0.0108 - acc: 0.9965\n",
            "Epoch 45/1000\n",
            "60000/60000 [==============================] - 25s 424us/step - loss: 0.0106 - acc: 0.9960\n",
            "Epoch 46/1000\n",
            "60000/60000 [==============================] - 25s 423us/step - loss: 0.0097 - acc: 0.9967\n",
            "Epoch 47/1000\n",
            "60000/60000 [==============================] - 25s 412us/step - loss: 0.0092 - acc: 0.9969\n",
            "Epoch 48/1000\n",
            "60000/60000 [==============================] - 26s 426us/step - loss: 0.0099 - acc: 0.9965\n",
            "Epoch 49/1000\n",
            "60000/60000 [==============================] - 26s 434us/step - loss: 0.0092 - acc: 0.9968\n",
            "Epoch 50/1000\n",
            "60000/60000 [==============================] - 26s 426us/step - loss: 0.0105 - acc: 0.9965\n",
            "Epoch 51/1000\n",
            "60000/60000 [==============================] - 27s 446us/step - loss: 0.0087 - acc: 0.9967\n",
            "Epoch 52/1000\n",
            "60000/60000 [==============================] - 26s 437us/step - loss: 0.0089 - acc: 0.9970\n",
            "Epoch 53/1000\n",
            "60000/60000 [==============================] - 26s 437us/step - loss: 0.0101 - acc: 0.9964\n",
            "Epoch 54/1000\n",
            "60000/60000 [==============================] - 28s 462us/step - loss: 0.0088 - acc: 0.9970\n",
            "Epoch 55/1000\n",
            "60000/60000 [==============================] - 27s 447us/step - loss: 0.0086 - acc: 0.9972\n",
            "Epoch 56/1000\n",
            "60000/60000 [==============================] - 28s 463us/step - loss: 0.0077 - acc: 0.9975\n",
            "Epoch 57/1000\n",
            "60000/60000 [==============================] - 26s 439us/step - loss: 0.0092 - acc: 0.9969\n",
            "Epoch 58/1000\n",
            "60000/60000 [==============================] - 26s 435us/step - loss: 0.0088 - acc: 0.9970\n",
            "Epoch 59/1000\n",
            "60000/60000 [==============================] - 26s 439us/step - loss: 0.0082 - acc: 0.9970\n",
            "Epoch 60/1000\n",
            "60000/60000 [==============================] - 26s 440us/step - loss: 0.0083 - acc: 0.9972\n",
            "Epoch 61/1000\n",
            "60000/60000 [==============================] - 27s 450us/step - loss: 0.0073 - acc: 0.9975\n",
            "Epoch 62/1000\n",
            "60000/60000 [==============================] - 26s 439us/step - loss: 0.0085 - acc: 0.9972\n",
            "Epoch 63/1000\n",
            "60000/60000 [==============================] - 26s 434us/step - loss: 0.0089 - acc: 0.9970\n",
            "Epoch 64/1000\n",
            "60000/60000 [==============================] - 26s 435us/step - loss: 0.0076 - acc: 0.9974\n",
            "Epoch 65/1000\n",
            "60000/60000 [==============================] - 26s 430us/step - loss: 0.0070 - acc: 0.9976\n",
            "Epoch 66/1000\n",
            "60000/60000 [==============================] - 27s 449us/step - loss: 0.0081 - acc: 0.9971\n",
            "Epoch 67/1000\n",
            "60000/60000 [==============================] - 27s 445us/step - loss: 0.0067 - acc: 0.9976\n",
            "Epoch 68/1000\n",
            "60000/60000 [==============================] - 27s 445us/step - loss: 0.0070 - acc: 0.9977\n",
            "Epoch 69/1000\n",
            "60000/60000 [==============================] - 26s 435us/step - loss: 0.0075 - acc: 0.9972\n",
            "Epoch 70/1000\n",
            "60000/60000 [==============================] - 26s 436us/step - loss: 0.0072 - acc: 0.9972\n",
            "Epoch 71/1000\n",
            "60000/60000 [==============================] - 27s 453us/step - loss: 0.0068 - acc: 0.9978\n",
            "Epoch 72/1000\n",
            "60000/60000 [==============================] - 27s 455us/step - loss: 0.0062 - acc: 0.9977\n",
            "Epoch 73/1000\n",
            "60000/60000 [==============================] - 26s 428us/step - loss: 0.0073 - acc: 0.9975\n",
            "Epoch 74/1000\n",
            "60000/60000 [==============================] - 25s 420us/step - loss: 0.0070 - acc: 0.9974\n",
            "Epoch 75/1000\n",
            "60000/60000 [==============================] - 25s 416us/step - loss: 0.0065 - acc: 0.9978\n",
            "Epoch 76/1000\n",
            "60000/60000 [==============================] - 25s 416us/step - loss: 0.0081 - acc: 0.9973\n",
            "Epoch 77/1000\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0063 - acc: 0.9977\n",
            "Epoch 78/1000\n",
            "60000/60000 [==============================] - 26s 426us/step - loss: 0.0064 - acc: 0.9975\n",
            "Epoch 79/1000\n",
            "60000/60000 [==============================] - 25s 419us/step - loss: 0.0068 - acc: 0.9976\n",
            "Epoch 80/1000\n",
            "60000/60000 [==============================] - 25s 417us/step - loss: 0.0071 - acc: 0.9976\n",
            "Epoch 81/1000\n",
            "60000/60000 [==============================] - 25s 422us/step - loss: 0.0066 - acc: 0.9977\n",
            "Epoch 82/1000\n",
            "60000/60000 [==============================] - 26s 427us/step - loss: 0.0071 - acc: 0.9975\n",
            "Epoch 83/1000\n",
            "60000/60000 [==============================] - 26s 432us/step - loss: 0.0069 - acc: 0.9976\n",
            "Epoch 84/1000\n",
            "60000/60000 [==============================] - 25s 416us/step - loss: 0.0060 - acc: 0.9979\n",
            "Epoch 85/1000\n",
            "60000/60000 [==============================] - 25s 424us/step - loss: 0.0059 - acc: 0.9979\n",
            "Epoch 86/1000\n",
            "60000/60000 [==============================] - 25s 420us/step - loss: 0.0068 - acc: 0.9977\n",
            "Epoch 87/1000\n",
            "60000/60000 [==============================] - 26s 434us/step - loss: 0.0066 - acc: 0.9976\n",
            "Epoch 88/1000\n",
            "60000/60000 [==============================] - 26s 426us/step - loss: 0.0065 - acc: 0.9976\n",
            "Epoch 89/1000\n",
            "60000/60000 [==============================] - 26s 435us/step - loss: 0.0052 - acc: 0.9981\n",
            "Epoch 90/1000\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0067 - acc: 0.9978\n",
            "Epoch 91/1000\n",
            "60000/60000 [==============================] - 25s 423us/step - loss: 0.0070 - acc: 0.9977\n",
            "Epoch 92/1000\n",
            "60000/60000 [==============================] - 27s 442us/step - loss: 0.0062 - acc: 0.9978\n",
            "Epoch 93/1000\n",
            "60000/60000 [==============================] - 26s 430us/step - loss: 0.0060 - acc: 0.9979\n",
            "Epoch 94/1000\n",
            "60000/60000 [==============================] - 26s 434us/step - loss: 0.0055 - acc: 0.9979\n",
            "Epoch 95/1000\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0062 - acc: 0.9977\n",
            "Epoch 96/1000\n",
            "60000/60000 [==============================] - 25s 419us/step - loss: 0.0061 - acc: 0.9980\n",
            "Epoch 97/1000\n",
            "60000/60000 [==============================] - 25s 411us/step - loss: 0.0051 - acc: 0.9982\n",
            "Epoch 98/1000\n",
            "60000/60000 [==============================] - 25s 421us/step - loss: 0.0062 - acc: 0.9978\n",
            "Epoch 99/1000\n",
            "60000/60000 [==============================] - 26s 427us/step - loss: 0.0064 - acc: 0.9978\n",
            "Epoch 100/1000\n",
            "60000/60000 [==============================] - 25s 412us/step - loss: 0.0059 - acc: 0.9979\n",
            "Epoch 101/1000\n",
            "60000/60000 [==============================] - 24s 403us/step - loss: 0.0059 - acc: 0.9980\n",
            "Epoch 102/1000\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0060 - acc: 0.9978\n",
            "Epoch 103/1000\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0050 - acc: 0.9983\n",
            "Epoch 104/1000\n",
            "60000/60000 [==============================] - 26s 428us/step - loss: 0.0058 - acc: 0.9980\n",
            "Epoch 105/1000\n",
            "60000/60000 [==============================] - 25s 416us/step - loss: 0.0055 - acc: 0.9981\n",
            "Epoch 106/1000\n",
            "60000/60000 [==============================] - 24s 402us/step - loss: 0.0062 - acc: 0.9978\n",
            "Epoch 107/1000\n",
            "60000/60000 [==============================] - 24s 399us/step - loss: 0.0052 - acc: 0.9979\n",
            "Epoch 108/1000\n",
            "60000/60000 [==============================] - 24s 399us/step - loss: 0.0038 - acc: 0.9988\n",
            "Epoch 109/1000\n",
            " 3904/60000 [>.............................] - ETA: 23s - loss: 0.0038 - acc: 0.9987"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d1be09664329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m              metrics=['accuracy'])\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYocqCKEHzLL",
        "colab_type": "text"
      },
      "source": [
        "## Saving the Weights for future reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vTCX7tcwSvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('weights_final.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEg0_1FTH5jT",
        "colab_type": "text"
      },
      "source": [
        "## Testing the Data and got an accuracy of 99.42"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JK07OIomzK5",
        "colab_type": "code",
        "outputId": "e9452046-d890-499b-db14-c88c1a7522b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred[:9])\n",
        "print(y_test[:9])\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "layer_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.030564815212788518, 0.9942]\n",
            "[[6.61670805e-13 2.85207730e-12 6.61670805e-13 2.23151055e-12\n",
            "  6.61670805e-13 6.61670805e-13 6.61670805e-13 1.00000000e+00\n",
            "  6.61670805e-13 6.25286541e-12]\n",
            " [1.47115315e-08 3.25124871e-10 9.99999523e-01 3.11163443e-12\n",
            "  3.11163443e-12 3.11163443e-12 4.78297295e-07 3.11163443e-12\n",
            "  3.11163443e-12 3.11163443e-12]\n",
            " [1.27132343e-12 1.00000000e+00 1.27132343e-12 1.27132343e-12\n",
            "  7.68470444e-12 1.27132343e-12 1.27132343e-12 2.50418575e-10\n",
            "  1.27132343e-12 1.27132343e-12]\n",
            " [9.99999762e-01 9.85648022e-12 2.57024679e-11 9.85648022e-12\n",
            "  9.85648022e-12 1.25399864e-11 2.03449375e-07 9.85648022e-12\n",
            "  4.92463258e-11 9.85648022e-12]\n",
            " [9.81114226e-17 7.70791141e-16 9.81114226e-17 9.81114226e-17\n",
            "  1.00000000e+00 9.81114226e-17 8.91514247e-16 9.81114226e-17\n",
            "  9.81114226e-17 2.37592888e-12]\n",
            " [2.91953256e-13 1.00000000e+00 2.91953256e-13 2.91953256e-13\n",
            "  6.83985583e-13 2.91953256e-13 2.91953256e-13 1.91590563e-11\n",
            "  2.91953256e-13 2.91953256e-13]\n",
            " [2.12045832e-12 2.12045832e-12 2.12045832e-12 2.12045832e-12\n",
            "  1.00000000e+00 2.12045832e-12 2.12045832e-12 2.12045832e-12\n",
            "  2.52831200e-10 3.56262442e-09]\n",
            " [2.48532098e-11 2.48532098e-11 3.06830047e-11 2.48532098e-11\n",
            "  2.34110012e-05 2.48532098e-11 2.48532098e-11 2.48532098e-11\n",
            "  1.30252019e-07 9.99976516e-01]\n",
            " [2.44713638e-10 1.86164127e-13 1.86164127e-13 1.86164127e-13\n",
            "  1.86164127e-13 9.99999404e-01 5.73360353e-07 1.86164127e-13\n",
            "  1.32792430e-10 1.86164127e-13]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation_12': <keras.layers.core.Activation at 0x7fe6556b6198>,\n",
              " 'batch_normalization_1': <keras.layers.normalization.BatchNormalization at 0x7fe655dfb2e8>,\n",
              " 'batch_normalization_2': <keras.layers.normalization.BatchNormalization at 0x7fe6556c94e0>,\n",
              " 'conv2d_114': <keras.layers.convolutional.Conv2D at 0x7fe655dfb0f0>,\n",
              " 'conv2d_115': <keras.layers.convolutional.Conv2D at 0x7fe655dfb0b8>,\n",
              " 'conv2d_116': <keras.layers.convolutional.Conv2D at 0x7fe655dfb358>,\n",
              " 'conv2d_117': <keras.layers.convolutional.Conv2D at 0x7fe655dfb940>,\n",
              " 'conv2d_118': <keras.layers.convolutional.Conv2D at 0x7fe65579b6a0>,\n",
              " 'conv2d_119': <keras.layers.convolutional.Conv2D at 0x7fe6556dd320>,\n",
              " 'dropout_5': <keras.layers.core.Dropout at 0x7fe655dfb9e8>,\n",
              " 'dropout_6': <keras.layers.core.Dropout at 0x7fe6556ddc50>,\n",
              " 'flatten_12': <keras.layers.core.Flatten at 0x7fe6556dd7f0>,\n",
              " 'max_pooling2d_35': <keras.layers.pooling.MaxPooling2D at 0x7fe6557e5358>,\n",
              " 'max_pooling2d_36': <keras.layers.pooling.MaxPooling2D at 0x7fe6556c9128>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW5eYicdIF3D",
        "colab_type": "text"
      },
      "source": [
        "## Loading the weights and retesting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDfzcCVSwjAD",
        "colab_type": "code",
        "outputId": "f14a5ee7-0580-497e-a3e5-5326775e4b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "model.load_weights('weights_final.h5')\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred[:9])\n",
        "print(y_test[:9])\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "layer_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.030564815212788518, 0.9942]\n",
            "[[6.61670805e-13 2.85207730e-12 6.61670805e-13 2.23151055e-12\n",
            "  6.61670805e-13 6.61670805e-13 6.61670805e-13 1.00000000e+00\n",
            "  6.61670805e-13 6.25286541e-12]\n",
            " [1.47115315e-08 3.25124871e-10 9.99999523e-01 3.11163443e-12\n",
            "  3.11163443e-12 3.11163443e-12 4.78297295e-07 3.11163443e-12\n",
            "  3.11163443e-12 3.11163443e-12]\n",
            " [1.27132343e-12 1.00000000e+00 1.27132343e-12 1.27132343e-12\n",
            "  7.68470444e-12 1.27132343e-12 1.27132343e-12 2.50418575e-10\n",
            "  1.27132343e-12 1.27132343e-12]\n",
            " [9.99999762e-01 9.85648022e-12 2.57024679e-11 9.85648022e-12\n",
            "  9.85648022e-12 1.25399864e-11 2.03449375e-07 9.85648022e-12\n",
            "  4.92463258e-11 9.85648022e-12]\n",
            " [9.81114226e-17 7.70791141e-16 9.81114226e-17 9.81114226e-17\n",
            "  1.00000000e+00 9.81114226e-17 8.91514247e-16 9.81114226e-17\n",
            "  9.81114226e-17 2.37592888e-12]\n",
            " [2.91953256e-13 1.00000000e+00 2.91953256e-13 2.91953256e-13\n",
            "  6.83985583e-13 2.91953256e-13 2.91953256e-13 1.91590563e-11\n",
            "  2.91953256e-13 2.91953256e-13]\n",
            " [2.12045832e-12 2.12045832e-12 2.12045832e-12 2.12045832e-12\n",
            "  1.00000000e+00 2.12045832e-12 2.12045832e-12 2.12045832e-12\n",
            "  2.52831200e-10 3.56262442e-09]\n",
            " [2.48532098e-11 2.48532098e-11 3.06830047e-11 2.48532098e-11\n",
            "  2.34110012e-05 2.48532098e-11 2.48532098e-11 2.48532098e-11\n",
            "  1.30252019e-07 9.99976516e-01]\n",
            " [2.44713638e-10 1.86164127e-13 1.86164127e-13 1.86164127e-13\n",
            "  1.86164127e-13 9.99999404e-01 5.73360353e-07 1.86164127e-13\n",
            "  1.32792430e-10 1.86164127e-13]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation_12': <keras.layers.core.Activation at 0x7fe6556b6198>,\n",
              " 'batch_normalization_1': <keras.layers.normalization.BatchNormalization at 0x7fe655dfb2e8>,\n",
              " 'batch_normalization_2': <keras.layers.normalization.BatchNormalization at 0x7fe6556c94e0>,\n",
              " 'conv2d_114': <keras.layers.convolutional.Conv2D at 0x7fe655dfb0f0>,\n",
              " 'conv2d_115': <keras.layers.convolutional.Conv2D at 0x7fe655dfb0b8>,\n",
              " 'conv2d_116': <keras.layers.convolutional.Conv2D at 0x7fe655dfb358>,\n",
              " 'conv2d_117': <keras.layers.convolutional.Conv2D at 0x7fe655dfb940>,\n",
              " 'conv2d_118': <keras.layers.convolutional.Conv2D at 0x7fe65579b6a0>,\n",
              " 'conv2d_119': <keras.layers.convolutional.Conv2D at 0x7fe6556dd320>,\n",
              " 'dropout_5': <keras.layers.core.Dropout at 0x7fe655dfb9e8>,\n",
              " 'dropout_6': <keras.layers.core.Dropout at 0x7fe6556ddc50>,\n",
              " 'flatten_12': <keras.layers.core.Flatten at 0x7fe6556dd7f0>,\n",
              " 'max_pooling2d_35': <keras.layers.pooling.MaxPooling2D at 0x7fe6557e5358>,\n",
              " 'max_pooling2d_36': <keras.layers.pooling.MaxPooling2D at 0x7fe6556c9128>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlKz1aiBIyda",
        "colab_type": "text"
      },
      "source": [
        "#--------------------END OF ASSIGNMENT 3----------------\n",
        "\n",
        "- #### Note: Continuing the same notebook for Assignment 4, the code below is still inprogress for Assignment4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN_xC--8VoB4",
        "colab_type": "code",
        "outputId": "6e82d19c-35de-46c5-d9a7-d1f6f9b5e40a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Convolution2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "\n",
        "y_train[:10]\n",
        "\n",
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "from keras.layers import Activation, Conv2D, MaxPooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Flatten\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, (3, 3) , activation='relu',use_bias=False, input_shape=(28,28,1)))  #26\n",
        "\n",
        "model.add(Conv2D(16, (3, 3) , activation='relu',use_bias=False))     # 24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(32, (3,3), activation = 'relu', use_bias= False))   # 22\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))      # 11, 11 \n",
        "\n",
        "model.add(Conv2D(16, (3,3), activation = 'relu', use_bias= False))   # 22\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(16, (3,3), activation = 'relu', use_bias= False))   # 22\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(10, (2,2), activation='relu',use_bias=False))          # 12x12, 10 layers\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1000, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 26, 26, 16)        144       \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 24, 24, 16)        2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 22, 22, 32)        4608      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 9, 9, 16)          4608      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 2, 2, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 2, 2, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 2, 2, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 1, 1, 10)          640       \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,736\n",
            "Trainable params: 14,672\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:63: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1000\n",
            "60000/60000 [==============================] - 23s 385us/step - loss: 0.2090 - acc: 0.9416 - val_loss: 0.0808 - val_acc: 0.9737\n",
            "Epoch 2/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0689 - acc: 0.9786 - val_loss: 0.0377 - val_acc: 0.9873\n",
            "Epoch 3/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0544 - acc: 0.9836 - val_loss: 0.0380 - val_acc: 0.9871\n",
            "Epoch 4/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0455 - acc: 0.9864 - val_loss: 0.0367 - val_acc: 0.9884\n",
            "Epoch 5/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0407 - acc: 0.9871 - val_loss: 0.0297 - val_acc: 0.9900\n",
            "Epoch 6/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0360 - acc: 0.9888 - val_loss: 0.0322 - val_acc: 0.9895\n",
            "Epoch 7/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0336 - acc: 0.9891 - val_loss: 0.0272 - val_acc: 0.9908\n",
            "Epoch 8/1000\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0307 - acc: 0.9901 - val_loss: 0.0256 - val_acc: 0.9926\n",
            "Epoch 9/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0285 - acc: 0.9908 - val_loss: 0.0419 - val_acc: 0.9879\n",
            "Epoch 10/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0268 - acc: 0.9913 - val_loss: 0.0353 - val_acc: 0.9879\n",
            "Epoch 11/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0260 - acc: 0.9914 - val_loss: 0.0293 - val_acc: 0.9904\n",
            "Epoch 12/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0238 - acc: 0.9917 - val_loss: 0.0252 - val_acc: 0.9924\n",
            "Epoch 13/1000\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0222 - acc: 0.9926 - val_loss: 0.0342 - val_acc: 0.9895\n",
            "Epoch 14/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0211 - acc: 0.9929 - val_loss: 0.0361 - val_acc: 0.9885\n",
            "Epoch 15/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0205 - acc: 0.9929 - val_loss: 0.0303 - val_acc: 0.9907\n",
            "Epoch 16/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0191 - acc: 0.9937 - val_loss: 0.0288 - val_acc: 0.9911\n",
            "Epoch 17/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0183 - acc: 0.9941 - val_loss: 0.0289 - val_acc: 0.9914\n",
            "Epoch 18/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0178 - acc: 0.9938 - val_loss: 0.0266 - val_acc: 0.9918\n",
            "Epoch 19/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0170 - acc: 0.9946 - val_loss: 0.0274 - val_acc: 0.9916\n",
            "Epoch 20/1000\n",
            "60000/60000 [==============================] - 23s 377us/step - loss: 0.0153 - acc: 0.9949 - val_loss: 0.0299 - val_acc: 0.9910\n",
            "Epoch 21/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0150 - acc: 0.9951 - val_loss: 0.0290 - val_acc: 0.9918\n",
            "Epoch 22/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0164 - acc: 0.9945 - val_loss: 0.0326 - val_acc: 0.9903\n",
            "Epoch 23/1000\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0149 - acc: 0.9951 - val_loss: 0.0256 - val_acc: 0.9915\n",
            "Epoch 24/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0141 - acc: 0.9952 - val_loss: 0.0275 - val_acc: 0.9911\n",
            "Epoch 25/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0133 - acc: 0.9957 - val_loss: 0.0285 - val_acc: 0.9913\n",
            "Epoch 26/1000\n",
            "60000/60000 [==============================] - 22s 373us/step - loss: 0.0140 - acc: 0.9952 - val_loss: 0.0358 - val_acc: 0.9904\n",
            "Epoch 27/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0126 - acc: 0.9958 - val_loss: 0.0297 - val_acc: 0.9915\n",
            "Epoch 28/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0124 - acc: 0.9957 - val_loss: 0.0317 - val_acc: 0.9913\n",
            "Epoch 29/1000\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0130 - acc: 0.9954 - val_loss: 0.0273 - val_acc: 0.9929\n",
            "Epoch 30/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0121 - acc: 0.9958 - val_loss: 0.0260 - val_acc: 0.9925\n",
            "Epoch 31/1000\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0114 - acc: 0.9959 - val_loss: 0.0335 - val_acc: 0.9919\n",
            "Epoch 32/1000\n",
            "60000/60000 [==============================] - 23s 375us/step - loss: 0.0108 - acc: 0.9964 - val_loss: 0.0324 - val_acc: 0.9912\n",
            "Epoch 33/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0107 - acc: 0.9963 - val_loss: 0.0280 - val_acc: 0.9934\n",
            "Epoch 34/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0103 - acc: 0.9962 - val_loss: 0.0384 - val_acc: 0.9901\n",
            "Epoch 35/1000\n",
            "60000/60000 [==============================] - 22s 374us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0279 - val_acc: 0.9913\n",
            "Epoch 36/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0107 - acc: 0.9962 - val_loss: 0.0242 - val_acc: 0.9931\n",
            "Epoch 37/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0102 - acc: 0.9964 - val_loss: 0.0283 - val_acc: 0.9919\n",
            "Epoch 38/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0099 - acc: 0.9966 - val_loss: 0.0312 - val_acc: 0.9924\n",
            "Epoch 39/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0433 - val_acc: 0.9897\n",
            "Epoch 40/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0090 - acc: 0.9967 - val_loss: 0.0341 - val_acc: 0.9917\n",
            "Epoch 41/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0092 - acc: 0.9969 - val_loss: 0.0305 - val_acc: 0.9919\n",
            "Epoch 42/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0087 - acc: 0.9969 - val_loss: 0.0288 - val_acc: 0.9919\n",
            "Epoch 43/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.0288 - val_acc: 0.9918\n",
            "Epoch 44/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0097 - acc: 0.9965 - val_loss: 0.0339 - val_acc: 0.9908\n",
            "Epoch 45/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0080 - acc: 0.9974 - val_loss: 0.0384 - val_acc: 0.9899\n",
            "Epoch 46/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0090 - acc: 0.9972 - val_loss: 0.0327 - val_acc: 0.9912\n",
            "Epoch 47/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0331 - val_acc: 0.9912\n",
            "Epoch 48/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0326 - val_acc: 0.9928\n",
            "Epoch 49/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0074 - acc: 0.9975 - val_loss: 0.0333 - val_acc: 0.9917\n",
            "Epoch 50/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0092 - acc: 0.9967 - val_loss: 0.0297 - val_acc: 0.9920\n",
            "Epoch 51/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0085 - acc: 0.9972 - val_loss: 0.0290 - val_acc: 0.9923\n",
            "Epoch 52/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0367 - val_acc: 0.9909\n",
            "Epoch 53/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0076 - acc: 0.9973 - val_loss: 0.0340 - val_acc: 0.9916\n",
            "Epoch 54/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0065 - acc: 0.9976 - val_loss: 0.0348 - val_acc: 0.9921\n",
            "Epoch 55/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0080 - acc: 0.9973 - val_loss: 0.0322 - val_acc: 0.9927\n",
            "Epoch 56/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.0304 - val_acc: 0.9927\n",
            "Epoch 57/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0302 - val_acc: 0.9924\n",
            "Epoch 58/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0074 - acc: 0.9973 - val_loss: 0.0314 - val_acc: 0.9920\n",
            "Epoch 59/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0072 - acc: 0.9975 - val_loss: 0.0311 - val_acc: 0.9926\n",
            "Epoch 60/1000\n",
            "60000/60000 [==============================] - 22s 373us/step - loss: 0.0067 - acc: 0.9978 - val_loss: 0.0344 - val_acc: 0.9916\n",
            "Epoch 61/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0066 - acc: 0.9977 - val_loss: 0.0272 - val_acc: 0.9935\n",
            "Epoch 62/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0275 - val_acc: 0.9937\n",
            "Epoch 63/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0290 - val_acc: 0.9920\n",
            "Epoch 64/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0061 - acc: 0.9977 - val_loss: 0.0349 - val_acc: 0.9915\n",
            "Epoch 65/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0341 - val_acc: 0.9923\n",
            "Epoch 66/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0337 - val_acc: 0.9928\n",
            "Epoch 67/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.0330 - val_acc: 0.9921\n",
            "Epoch 68/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0060 - acc: 0.9977 - val_loss: 0.0320 - val_acc: 0.9930\n",
            "Epoch 69/1000\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0062 - acc: 0.9980 - val_loss: 0.0326 - val_acc: 0.9932\n",
            "Epoch 70/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0062 - acc: 0.9976 - val_loss: 0.0366 - val_acc: 0.9916\n",
            "Epoch 71/1000\n",
            "60000/60000 [==============================] - 22s 373us/step - loss: 0.0060 - acc: 0.9980 - val_loss: 0.0308 - val_acc: 0.9933\n",
            "Epoch 72/1000\n",
            "60000/60000 [==============================] - 23s 380us/step - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0349 - val_acc: 0.9919\n",
            "Epoch 73/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0058 - acc: 0.9978 - val_loss: 0.0335 - val_acc: 0.9920\n",
            "Epoch 74/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0299 - val_acc: 0.9929\n",
            "Epoch 75/1000\n",
            "60000/60000 [==============================] - 22s 373us/step - loss: 0.0054 - acc: 0.9980 - val_loss: 0.0310 - val_acc: 0.9925\n",
            "Epoch 76/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0311 - val_acc: 0.9922\n",
            "Epoch 77/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0062 - acc: 0.9978 - val_loss: 0.0343 - val_acc: 0.9928\n",
            "Epoch 78/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0054 - acc: 0.9981 - val_loss: 0.0332 - val_acc: 0.9926\n",
            "Epoch 79/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0061 - acc: 0.9980 - val_loss: 0.0349 - val_acc: 0.9921\n",
            "Epoch 80/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0055 - acc: 0.9980 - val_loss: 0.0365 - val_acc: 0.9922\n",
            "Epoch 81/1000\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0371 - val_acc: 0.9914\n",
            "Epoch 82/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0052 - acc: 0.9980 - val_loss: 0.0305 - val_acc: 0.9930\n",
            "Epoch 83/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0350 - val_acc: 0.9923\n",
            "Epoch 84/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0059 - acc: 0.9982 - val_loss: 0.0347 - val_acc: 0.9928\n",
            "Epoch 85/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0049 - acc: 0.9980 - val_loss: 0.0347 - val_acc: 0.9921\n",
            "Epoch 86/1000\n",
            "60000/60000 [==============================] - 23s 375us/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0326 - val_acc: 0.9931\n",
            "Epoch 87/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0059 - acc: 0.9981 - val_loss: 0.0318 - val_acc: 0.9929\n",
            "Epoch 88/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0278 - val_acc: 0.9936\n",
            "Epoch 89/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0060 - acc: 0.9979 - val_loss: 0.0334 - val_acc: 0.9923\n",
            "Epoch 90/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0284 - val_acc: 0.9935\n",
            "Epoch 91/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0051 - acc: 0.9981 - val_loss: 0.0334 - val_acc: 0.9921\n",
            "Epoch 92/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0312 - val_acc: 0.9925\n",
            "Epoch 93/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0051 - acc: 0.9981 - val_loss: 0.0313 - val_acc: 0.9926\n",
            "Epoch 94/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0349 - val_acc: 0.9916\n",
            "Epoch 95/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0046 - acc: 0.9984 - val_loss: 0.0295 - val_acc: 0.9931\n",
            "Epoch 96/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0323 - val_acc: 0.9924\n",
            "Epoch 97/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0049 - acc: 0.9981 - val_loss: 0.0327 - val_acc: 0.9920\n",
            "Epoch 98/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0053 - acc: 0.9983 - val_loss: 0.0315 - val_acc: 0.9926\n",
            "Epoch 99/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0347 - val_acc: 0.9919\n",
            "Epoch 100/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0299 - val_acc: 0.9927\n",
            "Epoch 101/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0319 - val_acc: 0.9926\n",
            "Epoch 102/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0053 - acc: 0.9981 - val_loss: 0.0320 - val_acc: 0.9930\n",
            "Epoch 103/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0048 - acc: 0.9983 - val_loss: 0.0300 - val_acc: 0.9931\n",
            "Epoch 104/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0331 - val_acc: 0.9934\n",
            "Epoch 105/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0039 - acc: 0.9986 - val_loss: 0.0362 - val_acc: 0.9922\n",
            "Epoch 106/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0045 - acc: 0.9986 - val_loss: 0.0367 - val_acc: 0.9924\n",
            "Epoch 107/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0041 - acc: 0.9985 - val_loss: 0.0384 - val_acc: 0.9918\n",
            "Epoch 108/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0314 - val_acc: 0.9922\n",
            "Epoch 109/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0365 - val_acc: 0.9928\n",
            "Epoch 110/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0039 - acc: 0.9986 - val_loss: 0.0368 - val_acc: 0.9916\n",
            "Epoch 111/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.0350 - val_acc: 0.9927\n",
            "Epoch 112/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0044 - acc: 0.9984 - val_loss: 0.0394 - val_acc: 0.9911\n",
            "Epoch 113/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0046 - acc: 0.9983 - val_loss: 0.0296 - val_acc: 0.9941\n",
            "Epoch 114/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.0344 - val_acc: 0.9924\n",
            "Epoch 115/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0043 - acc: 0.9984 - val_loss: 0.0381 - val_acc: 0.9923\n",
            "Epoch 116/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0043 - acc: 0.9986 - val_loss: 0.0320 - val_acc: 0.9929\n",
            "Epoch 117/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0042 - acc: 0.9984 - val_loss: 0.0290 - val_acc: 0.9929\n",
            "Epoch 118/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0399 - val_acc: 0.9918\n",
            "Epoch 119/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0310 - val_acc: 0.9925\n",
            "Epoch 120/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0037 - acc: 0.9986 - val_loss: 0.0348 - val_acc: 0.9919\n",
            "Epoch 121/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0305 - val_acc: 0.9927\n",
            "Epoch 122/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0331 - val_acc: 0.9936\n",
            "Epoch 123/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0351 - val_acc: 0.9920\n",
            "Epoch 124/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0313 - val_acc: 0.9931\n",
            "Epoch 125/1000\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0040 - acc: 0.9986 - val_loss: 0.0333 - val_acc: 0.9925\n",
            "Epoch 126/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0041 - acc: 0.9985 - val_loss: 0.0346 - val_acc: 0.9932\n",
            "Epoch 127/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0040 - acc: 0.9987 - val_loss: 0.0308 - val_acc: 0.9937\n",
            "Epoch 128/1000\n",
            "60000/60000 [==============================] - 23s 378us/step - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0357 - val_acc: 0.9925\n",
            "Epoch 129/1000\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0043 - acc: 0.9984 - val_loss: 0.0286 - val_acc: 0.9933\n",
            "Epoch 130/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0289 - val_acc: 0.9930\n",
            "Epoch 131/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0314 - val_acc: 0.9922\n",
            "Epoch 132/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0317 - val_acc: 0.9926\n",
            "Epoch 133/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0044 - acc: 0.9984 - val_loss: 0.0367 - val_acc: 0.9918\n",
            "Epoch 134/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0302 - val_acc: 0.9933\n",
            "Epoch 135/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0275 - val_acc: 0.9935\n",
            "Epoch 136/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0314 - val_acc: 0.9928\n",
            "Epoch 137/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0356 - val_acc: 0.9931\n",
            "Epoch 138/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0041 - acc: 0.9985 - val_loss: 0.0283 - val_acc: 0.9939\n",
            "Epoch 139/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0325 - val_acc: 0.9930\n",
            "Epoch 140/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0345 - val_acc: 0.9928\n",
            "Epoch 141/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0344 - val_acc: 0.9929\n",
            "Epoch 142/1000\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0035 - acc: 0.9987 - val_loss: 0.0343 - val_acc: 0.9927\n",
            "Epoch 143/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0027 - acc: 0.9990 - val_loss: 0.0395 - val_acc: 0.9916\n",
            "Epoch 144/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0047 - acc: 0.9984 - val_loss: 0.0370 - val_acc: 0.9935\n",
            "Epoch 145/1000\n",
            "60000/60000 [==============================] - 22s 373us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0326 - val_acc: 0.9928\n",
            "Epoch 146/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.0313 - val_acc: 0.9930\n",
            "Epoch 147/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0299 - val_acc: 0.9932\n",
            "Epoch 148/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0403 - val_acc: 0.9913\n",
            "Epoch 149/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0032 - acc: 0.9987 - val_loss: 0.0328 - val_acc: 0.9934\n",
            "Epoch 150/1000\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0370 - val_acc: 0.9924\n",
            "Epoch 151/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0034 - acc: 0.9987 - val_loss: 0.0329 - val_acc: 0.9927\n",
            "Epoch 152/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0310 - val_acc: 0.9933\n",
            "Epoch 153/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0332 - val_acc: 0.9927\n",
            "Epoch 154/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0344 - val_acc: 0.9924\n",
            "Epoch 155/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0341 - val_acc: 0.9929\n",
            "Epoch 156/1000\n",
            "60000/60000 [==============================] - 22s 374us/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0352 - val_acc: 0.9928\n",
            "Epoch 157/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0041 - acc: 0.9985 - val_loss: 0.0334 - val_acc: 0.9928\n",
            "Epoch 158/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0333 - val_acc: 0.9927\n",
            "Epoch 159/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0325 - val_acc: 0.9933\n",
            "Epoch 160/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0391 - val_acc: 0.9919\n",
            "Epoch 161/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0034 - acc: 0.9987 - val_loss: 0.0415 - val_acc: 0.9915\n",
            "Epoch 162/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0323 - val_acc: 0.9929\n",
            "Epoch 163/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.0331 - val_acc: 0.9934\n",
            "Epoch 164/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0025 - acc: 0.9990 - val_loss: 0.0337 - val_acc: 0.9933\n",
            "Epoch 165/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0294 - val_acc: 0.9937\n",
            "Epoch 166/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.0389 - val_acc: 0.9916\n",
            "Epoch 167/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0374 - val_acc: 0.9933\n",
            "Epoch 168/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0373 - val_acc: 0.9920\n",
            "Epoch 169/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.0353 - val_acc: 0.9927\n",
            "Epoch 170/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0331 - val_acc: 0.9935\n",
            "Epoch 171/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0029 - acc: 0.9989 - val_loss: 0.0309 - val_acc: 0.9929\n",
            "Epoch 172/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0339 - val_acc: 0.9928\n",
            "Epoch 173/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0398 - val_acc: 0.9918\n",
            "Epoch 174/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0324 - val_acc: 0.9936\n",
            "Epoch 175/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0374 - val_acc: 0.9915\n",
            "Epoch 176/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0341 - val_acc: 0.9932\n",
            "Epoch 177/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0346 - val_acc: 0.9933\n",
            "Epoch 178/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0324 - val_acc: 0.9931\n",
            "Epoch 179/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0340 - val_acc: 0.9925\n",
            "Epoch 180/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0375 - val_acc: 0.9928\n",
            "Epoch 181/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0352 - val_acc: 0.9932\n",
            "Epoch 182/1000\n",
            "60000/60000 [==============================] - 21s 358us/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0341 - val_acc: 0.9926\n",
            "Epoch 183/1000\n",
            "60000/60000 [==============================] - 21s 357us/step - loss: 0.0029 - acc: 0.9989 - val_loss: 0.0299 - val_acc: 0.9931\n",
            "Epoch 184/1000\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.0324 - val_acc: 0.9928\n",
            "Epoch 185/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0332 - val_acc: 0.9935\n",
            "Epoch 186/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0304 - val_acc: 0.9941\n",
            "Epoch 187/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0296 - val_acc: 0.9932\n",
            "Epoch 188/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0030 - acc: 0.9989 - val_loss: 0.0318 - val_acc: 0.9936\n",
            "Epoch 189/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0340 - val_acc: 0.9928\n",
            "Epoch 190/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.0317 - val_acc: 0.9941\n",
            "Epoch 191/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.0347 - val_acc: 0.9933\n",
            "Epoch 192/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0366 - val_acc: 0.9934\n",
            "Epoch 193/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0023 - acc: 0.9990 - val_loss: 0.0303 - val_acc: 0.9933\n",
            "Epoch 194/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0383 - val_acc: 0.9922\n",
            "Epoch 195/1000\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0346 - val_acc: 0.9927\n",
            "Epoch 196/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0352 - val_acc: 0.9934\n",
            "Epoch 197/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0351 - val_acc: 0.9931\n",
            "Epoch 198/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0355 - val_acc: 0.9926\n",
            "Epoch 199/1000\n",
            "60000/60000 [==============================] - 22s 374us/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0370 - val_acc: 0.9931\n",
            "Epoch 200/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0369 - val_acc: 0.9928\n",
            "Epoch 201/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0326 - val_acc: 0.9933\n",
            "Epoch 202/1000\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0345 - val_acc: 0.9925\n",
            "Epoch 203/1000\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0319 - val_acc: 0.9929\n",
            "Epoch 204/1000\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0319 - val_acc: 0.9928\n",
            "Epoch 205/1000\n",
            "60000/60000 [==============================] - 21s 358us/step - loss: 0.0030 - acc: 0.9989 - val_loss: 0.0375 - val_acc: 0.9924\n",
            "Epoch 206/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0318 - val_acc: 0.9929\n",
            "Epoch 207/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.0355 - val_acc: 0.9926\n",
            "Epoch 208/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0356 - val_acc: 0.9932\n",
            "Epoch 209/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0301 - val_acc: 0.9931\n",
            "Epoch 210/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0338 - val_acc: 0.9931\n",
            "Epoch 211/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0331 - val_acc: 0.9926\n",
            "Epoch 212/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0320 - val_acc: 0.9929\n",
            "Epoch 213/1000\n",
            "60000/60000 [==============================] - 22s 375us/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0315 - val_acc: 0.9939\n",
            "Epoch 214/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0367 - val_acc: 0.9923\n",
            "Epoch 215/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0027 - acc: 0.9990 - val_loss: 0.0336 - val_acc: 0.9932\n",
            "Epoch 216/1000\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0025 - acc: 0.9990 - val_loss: 0.0322 - val_acc: 0.9923\n",
            "Epoch 217/1000\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0370 - val_acc: 0.9923\n",
            "Epoch 218/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0293 - val_acc: 0.9941\n",
            "Epoch 219/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0311 - val_acc: 0.9939\n",
            "Epoch 220/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0376 - val_acc: 0.9923\n",
            "Epoch 221/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0375 - val_acc: 0.9929\n",
            "Epoch 222/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0374 - val_acc: 0.9919\n",
            "Epoch 223/1000\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0360 - val_acc: 0.9933\n",
            "Epoch 224/1000\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0394 - val_acc: 0.9922\n",
            "Epoch 225/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0357 - val_acc: 0.9931\n",
            "Epoch 226/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0354 - val_acc: 0.9923\n",
            "Epoch 227/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0359 - val_acc: 0.9928\n",
            "Epoch 228/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0312 - val_acc: 0.9935\n",
            "Epoch 229/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0346 - val_acc: 0.9933\n",
            "Epoch 230/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0306 - val_acc: 0.9941\n",
            "Epoch 231/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0301 - val_acc: 0.9938\n",
            "Epoch 232/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0366 - val_acc: 0.9933\n",
            "Epoch 233/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0323 - val_acc: 0.9939\n",
            "Epoch 234/1000\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0024 - acc: 0.9990 - val_loss: 0.0355 - val_acc: 0.9924\n",
            "Epoch 235/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0358 - val_acc: 0.9933\n",
            "Epoch 236/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0029 - acc: 0.9989 - val_loss: 0.0351 - val_acc: 0.9932\n",
            "Epoch 237/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0351 - val_acc: 0.9927\n",
            "Epoch 238/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0341 - val_acc: 0.9934\n",
            "Epoch 239/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0360 - val_acc: 0.9937\n",
            "Epoch 240/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0373 - val_acc: 0.9929\n",
            "Epoch 241/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0350 - val_acc: 0.9940\n",
            "Epoch 242/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0437 - val_acc: 0.9925\n",
            "Epoch 243/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0405 - val_acc: 0.9919\n",
            "Epoch 244/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0358 - val_acc: 0.9936\n",
            "Epoch 245/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0344 - val_acc: 0.9937\n",
            "Epoch 246/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0327 - val_acc: 0.9932\n",
            "Epoch 247/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0366 - val_acc: 0.9929\n",
            "Epoch 248/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0393 - val_acc: 0.9924\n",
            "Epoch 249/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0391 - val_acc: 0.9923\n",
            "Epoch 250/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0022 - acc: 0.9991 - val_loss: 0.0358 - val_acc: 0.9937\n",
            "Epoch 251/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0352 - val_acc: 0.9929\n",
            "Epoch 252/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0371 - val_acc: 0.9931\n",
            "Epoch 253/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0351 - val_acc: 0.9933\n",
            "Epoch 254/1000\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.0369 - val_acc: 0.9931\n",
            "Epoch 255/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0356 - val_acc: 0.9930\n",
            "Epoch 256/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0392 - val_acc: 0.9926\n",
            "Epoch 257/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0379 - val_acc: 0.9932\n",
            "Epoch 258/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0365 - val_acc: 0.9932\n",
            "Epoch 259/1000\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0400 - val_acc: 0.9924\n",
            "Epoch 260/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0374 - val_acc: 0.9924\n",
            "Epoch 261/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0399 - val_acc: 0.9930\n",
            "Epoch 262/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0421 - val_acc: 0.9922\n",
            "Epoch 263/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0364 - val_acc: 0.9933\n",
            "Epoch 264/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0428 - val_acc: 0.9918\n",
            "Epoch 265/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0377 - val_acc: 0.9937\n",
            "Epoch 266/1000\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0350 - val_acc: 0.9930\n",
            "Epoch 267/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0027 - acc: 0.9990 - val_loss: 0.0398 - val_acc: 0.9926\n",
            "Epoch 268/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0369 - val_acc: 0.9932\n",
            "Epoch 269/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0358 - val_acc: 0.9930\n",
            "Epoch 270/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0312 - val_acc: 0.9932\n",
            "Epoch 271/1000\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0326 - val_acc: 0.9934\n",
            "Epoch 272/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0334 - val_acc: 0.9933\n",
            "Epoch 273/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0352 - val_acc: 0.9935\n",
            "Epoch 274/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0375 - val_acc: 0.9926\n",
            "Epoch 275/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0408 - val_acc: 0.9926\n",
            "Epoch 276/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0385 - val_acc: 0.9928\n",
            "Epoch 277/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0330 - val_acc: 0.9930\n",
            "Epoch 278/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0363 - val_acc: 0.9929\n",
            "Epoch 279/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0402 - val_acc: 0.9933\n",
            "Epoch 280/1000\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0390 - val_acc: 0.9931\n",
            "Epoch 281/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0366 - val_acc: 0.9930\n",
            "Epoch 282/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0397 - val_acc: 0.9936\n",
            "Epoch 283/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0428 - val_acc: 0.9923\n",
            "Epoch 284/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0369 - val_acc: 0.9934\n",
            "Epoch 285/1000\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0408 - val_acc: 0.9929\n",
            "Epoch 286/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0373 - val_acc: 0.9938\n",
            "Epoch 287/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0413 - val_acc: 0.9926\n",
            "Epoch 288/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0020 - acc: 0.9992 - val_loss: 0.0368 - val_acc: 0.9927\n",
            "Epoch 289/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0019 - acc: 0.9992 - val_loss: 0.0364 - val_acc: 0.9938\n",
            "Epoch 290/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0339 - val_acc: 0.9936\n",
            "Epoch 291/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0027 - acc: 0.9990 - val_loss: 0.0353 - val_acc: 0.9934\n",
            "Epoch 292/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0365 - val_acc: 0.9925\n",
            "Epoch 293/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0400 - val_acc: 0.9923\n",
            "Epoch 294/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0389 - val_acc: 0.9924\n",
            "Epoch 295/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0385 - val_acc: 0.9929\n",
            "Epoch 296/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0312 - val_acc: 0.9938\n",
            "Epoch 297/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0302 - val_acc: 0.9937\n",
            "Epoch 298/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0377 - val_acc: 0.9924\n",
            "Epoch 299/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0341 - val_acc: 0.9937\n",
            "Epoch 300/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0371 - val_acc: 0.9928\n",
            "Epoch 301/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0347 - val_acc: 0.9936\n",
            "Epoch 302/1000\n",
            "60000/60000 [==============================] - 22s 371us/step - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0344 - val_acc: 0.9933\n",
            "Epoch 303/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0310 - val_acc: 0.9943\n",
            "Epoch 304/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0357 - val_acc: 0.9934\n",
            "Epoch 305/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0343 - val_acc: 0.9934\n",
            "Epoch 306/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0351 - val_acc: 0.9938\n",
            "Epoch 307/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0346 - val_acc: 0.9938\n",
            "Epoch 308/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0321 - val_acc: 0.9939\n",
            "Epoch 309/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0354 - val_acc: 0.9936\n",
            "Epoch 310/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0358 - val_acc: 0.9939\n",
            "Epoch 311/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0357 - val_acc: 0.9931\n",
            "Epoch 312/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0020 - acc: 0.9992 - val_loss: 0.0367 - val_acc: 0.9935\n",
            "Epoch 313/1000\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0353 - val_acc: 0.9930\n",
            "Epoch 314/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0336 - val_acc: 0.9933\n",
            "Epoch 315/1000\n",
            "60000/60000 [==============================] - 22s 369us/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0368 - val_acc: 0.9929\n",
            "Epoch 316/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0338 - val_acc: 0.9937\n",
            "Epoch 317/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0335 - val_acc: 0.9929\n",
            "Epoch 318/1000\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0351 - val_acc: 0.9934\n",
            "Epoch 319/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0381 - val_acc: 0.9926\n",
            "Epoch 320/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0362 - val_acc: 0.9930\n",
            "Epoch 321/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0342 - val_acc: 0.9937\n",
            "Epoch 322/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0359 - val_acc: 0.9935\n",
            "Epoch 323/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0370 - val_acc: 0.9938\n",
            "Epoch 324/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0344 - val_acc: 0.9941\n",
            "Epoch 325/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0338 - val_acc: 0.9937\n",
            "Epoch 326/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0366 - val_acc: 0.9927\n",
            "Epoch 327/1000\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0341 - val_acc: 0.9937\n",
            "Epoch 328/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0360 - val_acc: 0.9937\n",
            "Epoch 329/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0337 - val_acc: 0.9937\n",
            "Epoch 330/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0396 - val_acc: 0.9925\n",
            "Epoch 331/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0337 - val_acc: 0.9941\n",
            "Epoch 332/1000\n",
            "60000/60000 [==============================] - 22s 374us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0401 - val_acc: 0.9932\n",
            "Epoch 333/1000\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0340 - val_acc: 0.9939\n",
            "Epoch 334/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0334 - val_acc: 0.9940\n",
            "Epoch 335/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0325 - val_acc: 0.9941\n",
            "Epoch 336/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0388 - val_acc: 0.9928\n",
            "Epoch 337/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0324 - val_acc: 0.9935\n",
            "Epoch 338/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0372 - val_acc: 0.9927\n",
            "Epoch 339/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0348 - val_acc: 0.9940\n",
            "Epoch 340/1000\n",
            "60000/60000 [==============================] - 22s 360us/step - loss: 0.0018 - acc: 0.9993 - val_loss: 0.0347 - val_acc: 0.9932\n",
            "Epoch 341/1000\n",
            "60000/60000 [==============================] - 22s 374us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0354 - val_acc: 0.9929\n",
            "Epoch 342/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0361 - val_acc: 0.9933\n",
            "Epoch 343/1000\n",
            "60000/60000 [==============================] - 22s 365us/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0369 - val_acc: 0.9930\n",
            "Epoch 344/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0018 - acc: 0.9993 - val_loss: 0.0374 - val_acc: 0.9925\n",
            "Epoch 345/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0380 - val_acc: 0.9935\n",
            "Epoch 346/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0365 - val_acc: 0.9934\n",
            "Epoch 347/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0395 - val_acc: 0.9922\n",
            "Epoch 348/1000\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0018 - acc: 0.9993 - val_loss: 0.0361 - val_acc: 0.9935\n",
            "Epoch 349/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0366 - val_acc: 0.9931\n",
            "Epoch 350/1000\n",
            "60000/60000 [==============================] - 21s 357us/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0380 - val_acc: 0.9929\n",
            "Epoch 351/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0027 - acc: 0.9990 - val_loss: 0.0352 - val_acc: 0.9935\n",
            "Epoch 352/1000\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0360 - val_acc: 0.9929\n",
            "Epoch 353/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0375 - val_acc: 0.9929\n",
            "Epoch 354/1000\n",
            "60000/60000 [==============================] - 21s 357us/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0346 - val_acc: 0.9935\n",
            "Epoch 355/1000\n",
            "60000/60000 [==============================] - 22s 370us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0386 - val_acc: 0.9925\n",
            "Epoch 356/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0391 - val_acc: 0.9927\n",
            "Epoch 357/1000\n",
            "60000/60000 [==============================] - 22s 364us/step - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0363 - val_acc: 0.9940\n",
            "Epoch 358/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0397 - val_acc: 0.9930\n",
            "Epoch 359/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0399 - val_acc: 0.9922\n",
            "Epoch 360/1000\n",
            "60000/60000 [==============================] - 22s 361us/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0415 - val_acc: 0.9929\n",
            "Epoch 361/1000\n",
            "60000/60000 [==============================] - 21s 358us/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0391 - val_acc: 0.9932\n",
            "Epoch 362/1000\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0394 - val_acc: 0.9933\n",
            "Epoch 363/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0364 - val_acc: 0.9934\n",
            "Epoch 364/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0356 - val_acc: 0.9926\n",
            "Epoch 365/1000\n",
            "60000/60000 [==============================] - 22s 362us/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0367 - val_acc: 0.9933\n",
            "Epoch 366/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0344 - val_acc: 0.9938\n",
            "Epoch 367/1000\n",
            "60000/60000 [==============================] - 22s 367us/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0373 - val_acc: 0.9936\n",
            "Epoch 368/1000\n",
            "60000/60000 [==============================] - 22s 366us/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0357 - val_acc: 0.9933\n",
            "Epoch 369/1000\n",
            "60000/60000 [==============================] - 22s 363us/step - loss: 0.0017 - acc: 0.9993 - val_loss: 0.0346 - val_acc: 0.9939\n",
            "Epoch 370/1000\n",
            "60000/60000 [==============================] - 22s 368us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0347 - val_acc: 0.9938\n",
            "Epoch 371/1000\n",
            "60000/60000 [==============================] - 22s 359us/step - loss: 0.0020 - acc: 0.9992 - val_loss: 0.0320 - val_acc: 0.9940\n",
            "Epoch 372/1000\n",
            "42464/60000 [====================>.........] - ETA: 6s - loss: 0.0018 - acc: 0.9993"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-717af7d27610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m              metrics=['accuracy'])\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzzPyYhVLO-_",
        "colab_type": "code",
        "outputId": "dabea19a-68ab-4cc2-b73a-155c90f73343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred[:9])\n",
        "print(y_test[:9])\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "layer_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.036095355841147606, 0.9933]\n",
            "[[3.49206623e-12 3.49206623e-12 3.49206623e-12 3.49206623e-12\n",
            "  3.49206623e-12 3.49206623e-12 3.49206623e-12 1.00000000e+00\n",
            "  3.49206623e-12 3.49206623e-12]\n",
            " [1.20606411e-12 9.51203905e-14 1.00000000e+00 4.21583036e-15\n",
            "  4.21583036e-15 4.21583036e-15 1.02455639e-11 4.21583036e-15\n",
            "  4.21583036e-15 4.21583036e-15]\n",
            " [6.91294208e-15 1.00000000e+00 6.91294208e-15 6.91294208e-15\n",
            "  7.46471114e-12 6.91294208e-15 7.21596177e-15 2.93267945e-11\n",
            "  6.91294208e-15 6.91294208e-15]\n",
            " [1.00000000e+00 6.90816381e-17 6.90816381e-17 6.90816381e-17\n",
            "  6.90816381e-17 4.44097839e-16 9.00082960e-12 6.90816381e-17\n",
            "  3.37749102e-14 8.53838791e-17]\n",
            " [4.56599562e-19 4.56599562e-19 4.56599562e-19 4.56599562e-19\n",
            "  1.00000000e+00 4.56599562e-19 4.56599562e-19 1.94526824e-18\n",
            "  4.56599562e-19 1.05531003e-12]\n",
            " [2.07790853e-14 1.00000000e+00 2.07790853e-14 2.07790853e-14\n",
            "  7.41501652e-12 2.07790853e-14 2.07790853e-14 4.28917700e-11\n",
            "  2.07790853e-14 2.07790853e-14]\n",
            " [6.10081325e-15 2.35889081e-13 6.10081325e-15 6.10081325e-15\n",
            "  1.00000000e+00 6.10081325e-15 6.10081325e-15 7.97166153e-12\n",
            "  1.20152818e-14 2.65143796e-10]\n",
            " [1.94935146e-13 1.94935146e-13 9.46424905e-13 1.94935146e-13\n",
            "  1.15057555e-08 1.94935146e-13 1.94935146e-13 1.94935146e-13\n",
            "  5.16909918e-11 1.00000000e+00]\n",
            " [1.52617801e-17 1.52617801e-17 1.52617801e-17 1.52617801e-17\n",
            "  1.52617801e-17 1.00000000e+00 1.06777412e-12 1.52617801e-17\n",
            "  4.30355404e-13 1.52617801e-17]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation_2': <keras.layers.core.Activation at 0x7f3841faf7f0>,\n",
              " 'batch_normalization_5': <keras.layers.normalization.BatchNormalization at 0x7f3844a28be0>,\n",
              " 'batch_normalization_6': <keras.layers.normalization.BatchNormalization at 0x7f3841ff0390>,\n",
              " 'conv2d_13': <keras.layers.convolutional.Conv2D at 0x7f3844a281d0>,\n",
              " 'conv2d_14': <keras.layers.convolutional.Conv2D at 0x7f3844a28da0>,\n",
              " 'conv2d_15': <keras.layers.convolutional.Conv2D at 0x7f3844a18630>,\n",
              " 'conv2d_16': <keras.layers.convolutional.Conv2D at 0x7f3844a8a208>,\n",
              " 'conv2d_17': <keras.layers.convolutional.Conv2D at 0x7f38c7f036d8>,\n",
              " 'conv2d_18': <keras.layers.convolutional.Conv2D at 0x7f3842003e48>,\n",
              " 'dropout_5': <keras.layers.core.Dropout at 0x7f3844a8a1d0>,\n",
              " 'dropout_6': <keras.layers.core.Dropout at 0x7f3841ff0d30>,\n",
              " 'flatten_2': <keras.layers.core.Flatten at 0x7f3841fc9f60>,\n",
              " 'max_pooling2d_5': <keras.layers.pooling.MaxPooling2D at 0x7f38449d89e8>,\n",
              " 'max_pooling2d_6': <keras.layers.pooling.MaxPooling2D at 0x7f3841ff0320>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDh2Kg7d89aN",
        "colab_type": "code",
        "outputId": "04992533-7fb7-4782-e2f9-b30a531fc922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation, Conv2D, MaxPooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Conv2D(32, (3, 3) , activation='relu',use_bias=False, input_shape=(28,28,1)))\n",
        "model.add(Conv2D(60, (3, 3) , activation='relu',use_bias=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
        "model.add(Conv2D(10, 1, activation='relu',use_bias=False))\n",
        "model.add(Conv2D(4, (3, 3) , activation='relu',use_bias=False))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(10, 5, activation='relu',use_bias=False))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1000, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        288       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 60)        17280     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 60)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 60)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 10)        600       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 4)         360       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 4)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 1, 1, 10)          1000      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 19,528\n",
            "Trainable params: 19,528\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "60000/60000 [==============================] - 22s 373us/step - loss: 0.4694 - acc: 0.8372\n",
            "Epoch 2/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.3187 - acc: 0.8775\n",
            "Epoch 3/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2980 - acc: 0.8824\n",
            "Epoch 4/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2870 - acc: 0.8859\n",
            "Epoch 5/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2815 - acc: 0.8876\n",
            "Epoch 6/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2764 - acc: 0.8886\n",
            "Epoch 7/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2733 - acc: 0.8900\n",
            "Epoch 8/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2705 - acc: 0.8899\n",
            "Epoch 9/1000\n",
            "60000/60000 [==============================] - 21s 350us/step - loss: 0.2692 - acc: 0.8906\n",
            "Epoch 10/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2665 - acc: 0.8915\n",
            "Epoch 11/1000\n",
            "60000/60000 [==============================] - 21s 346us/step - loss: 0.2654 - acc: 0.8913\n",
            "Epoch 12/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2642 - acc: 0.8923\n",
            "Epoch 13/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2628 - acc: 0.8922\n",
            "Epoch 14/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2619 - acc: 0.8923\n",
            "Epoch 15/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2602 - acc: 0.8928\n",
            "Epoch 16/1000\n",
            "60000/60000 [==============================] - 21s 347us/step - loss: 0.2598 - acc: 0.8928\n",
            "Epoch 17/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2581 - acc: 0.8933\n",
            "Epoch 18/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2565 - acc: 0.8936\n",
            "Epoch 19/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2565 - acc: 0.8936\n",
            "Epoch 20/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2557 - acc: 0.8939\n",
            "Epoch 21/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2541 - acc: 0.8945\n",
            "Epoch 22/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2543 - acc: 0.8945\n",
            "Epoch 23/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2537 - acc: 0.8943\n",
            "Epoch 24/1000\n",
            "60000/60000 [==============================] - 21s 349us/step - loss: 0.2523 - acc: 0.8951\n",
            "Epoch 25/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2509 - acc: 0.8959\n",
            "Epoch 26/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2510 - acc: 0.8953\n",
            "Epoch 27/1000\n",
            "60000/60000 [==============================] - 20s 342us/step - loss: 0.2508 - acc: 0.8955\n",
            "Epoch 28/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2492 - acc: 0.8956\n",
            "Epoch 29/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2503 - acc: 0.8952\n",
            "Epoch 30/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2481 - acc: 0.8960\n",
            "Epoch 31/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2482 - acc: 0.8957\n",
            "Epoch 32/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2481 - acc: 0.8956\n",
            "Epoch 33/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2476 - acc: 0.8960\n",
            "Epoch 34/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2454 - acc: 0.8962\n",
            "Epoch 35/1000\n",
            "60000/60000 [==============================] - 21s 348us/step - loss: 0.2468 - acc: 0.8962\n",
            "Epoch 36/1000\n",
            "60000/60000 [==============================] - 21s 348us/step - loss: 0.2458 - acc: 0.8961\n",
            "Epoch 37/1000\n",
            "60000/60000 [==============================] - 21s 348us/step - loss: 0.2462 - acc: 0.8963\n",
            "Epoch 38/1000\n",
            "60000/60000 [==============================] - 21s 347us/step - loss: 0.2442 - acc: 0.8969\n",
            "Epoch 39/1000\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.2454 - acc: 0.8964\n",
            "Epoch 40/1000\n",
            "60000/60000 [==============================] - 21s 346us/step - loss: 0.2444 - acc: 0.8969\n",
            "Epoch 41/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2429 - acc: 0.8975\n",
            "Epoch 42/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2446 - acc: 0.8965\n",
            "Epoch 43/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2428 - acc: 0.8972\n",
            "Epoch 44/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2424 - acc: 0.8974\n",
            "Epoch 45/1000\n",
            "60000/60000 [==============================] - 21s 346us/step - loss: 0.2436 - acc: 0.8970\n",
            "Epoch 46/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2420 - acc: 0.8976\n",
            "Epoch 47/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2423 - acc: 0.8972\n",
            "Epoch 48/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2420 - acc: 0.8977\n",
            "Epoch 49/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2420 - acc: 0.8973\n",
            "Epoch 50/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2419 - acc: 0.8975\n",
            "Epoch 51/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2415 - acc: 0.8976\n",
            "Epoch 52/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2424 - acc: 0.8972\n",
            "Epoch 53/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2401 - acc: 0.8980\n",
            "Epoch 54/1000\n",
            "60000/60000 [==============================] - 21s 349us/step - loss: 0.2413 - acc: 0.8978\n",
            "Epoch 55/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2396 - acc: 0.8983\n",
            "Epoch 56/1000\n",
            "60000/60000 [==============================] - 21s 346us/step - loss: 0.2414 - acc: 0.8976\n",
            "Epoch 57/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2400 - acc: 0.8982\n",
            "Epoch 58/1000\n",
            "60000/60000 [==============================] - 21s 346us/step - loss: 0.2397 - acc: 0.8982\n",
            "Epoch 59/1000\n",
            "60000/60000 [==============================] - 20s 342us/step - loss: 0.2404 - acc: 0.8979\n",
            "Epoch 60/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2389 - acc: 0.8980\n",
            "Epoch 61/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2389 - acc: 0.8985\n",
            "Epoch 62/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2389 - acc: 0.8984\n",
            "Epoch 63/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2396 - acc: 0.8979\n",
            "Epoch 64/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2379 - acc: 0.8986\n",
            "Epoch 65/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2397 - acc: 0.8979\n",
            "Epoch 66/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2387 - acc: 0.8983\n",
            "Epoch 67/1000\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.2383 - acc: 0.8984\n",
            "Epoch 68/1000\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.2381 - acc: 0.8985\n",
            "Epoch 69/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2382 - acc: 0.8983\n",
            "Epoch 70/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2385 - acc: 0.8985\n",
            "Epoch 71/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2387 - acc: 0.8984\n",
            "Epoch 72/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2383 - acc: 0.8984\n",
            "Epoch 73/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2382 - acc: 0.8985\n",
            "Epoch 74/1000\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.2377 - acc: 0.8987\n",
            "Epoch 75/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2394 - acc: 0.8983\n",
            "Epoch 76/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2361 - acc: 0.8992\n",
            "Epoch 77/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2371 - acc: 0.8991\n",
            "Epoch 78/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2372 - acc: 0.8991\n",
            "Epoch 79/1000\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.2371 - acc: 0.8991\n",
            "Epoch 80/1000\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.2370 - acc: 0.8991\n",
            "Epoch 81/1000\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.2378 - acc: 0.8985\n",
            "Epoch 82/1000\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.2382 - acc: 0.8988\n",
            "Epoch 83/1000\n",
            "60000/60000 [==============================] - 20s 333us/step - loss: 0.2379 - acc: 0.8986\n",
            "Epoch 84/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2370 - acc: 0.8989\n",
            "Epoch 85/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2374 - acc: 0.8985\n",
            "Epoch 86/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2366 - acc: 0.8989\n",
            "Epoch 87/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2372 - acc: 0.8989\n",
            "Epoch 88/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2372 - acc: 0.8988\n",
            "Epoch 89/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2360 - acc: 0.8991\n",
            "Epoch 90/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2377 - acc: 0.8986\n",
            "Epoch 91/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2369 - acc: 0.8988\n",
            "Epoch 92/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2351 - acc: 0.8994\n",
            "Epoch 93/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2379 - acc: 0.8987\n",
            "Epoch 94/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2355 - acc: 0.8994\n",
            "Epoch 95/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2376 - acc: 0.8986\n",
            "Epoch 96/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2367 - acc: 0.8991\n",
            "Epoch 97/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2352 - acc: 0.8996\n",
            "Epoch 98/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2362 - acc: 0.8989\n",
            "Epoch 99/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2368 - acc: 0.8991\n",
            "Epoch 100/1000\n",
            "60000/60000 [==============================] - 21s 346us/step - loss: 0.2369 - acc: 0.8990\n",
            "Epoch 101/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2361 - acc: 0.8994\n",
            "Epoch 102/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2357 - acc: 0.8994\n",
            "Epoch 103/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2366 - acc: 0.8990\n",
            "Epoch 104/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2355 - acc: 0.8994\n",
            "Epoch 105/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2361 - acc: 0.8990\n",
            "Epoch 106/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2355 - acc: 0.8994\n",
            "Epoch 107/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2365 - acc: 0.8992\n",
            "Epoch 108/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2341 - acc: 0.8998\n",
            "Epoch 109/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2356 - acc: 0.8992\n",
            "Epoch 110/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2363 - acc: 0.8992\n",
            "Epoch 111/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2338 - acc: 0.8996\n",
            "Epoch 112/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2357 - acc: 0.8994\n",
            "Epoch 113/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2371 - acc: 0.8991\n",
            "Epoch 114/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2360 - acc: 0.8992\n",
            "Epoch 115/1000\n",
            "60000/60000 [==============================] - 21s 347us/step - loss: 0.2365 - acc: 0.8991\n",
            "Epoch 116/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2364 - acc: 0.8992\n",
            "Epoch 117/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2346 - acc: 0.8996\n",
            "Epoch 118/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2353 - acc: 0.8996\n",
            "Epoch 119/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2346 - acc: 0.8998\n",
            "Epoch 120/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2365 - acc: 0.8993\n",
            "Epoch 121/1000\n",
            "60000/60000 [==============================] - 20s 333us/step - loss: 0.2349 - acc: 0.8995\n",
            "Epoch 122/1000\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 0.2374 - acc: 0.8990\n",
            "Epoch 123/1000\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 0.2353 - acc: 0.8993\n",
            "Epoch 124/1000\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 0.2359 - acc: 0.8992\n",
            "Epoch 125/1000\n",
            "60000/60000 [==============================] - 20s 329us/step - loss: 0.2341 - acc: 0.8998\n",
            "Epoch 126/1000\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 0.2353 - acc: 0.8996\n",
            "Epoch 127/1000\n",
            "60000/60000 [==============================] - 20s 328us/step - loss: 0.2350 - acc: 0.8996\n",
            "Epoch 128/1000\n",
            "60000/60000 [==============================] - 20s 330us/step - loss: 0.2345 - acc: 0.8996\n",
            "Epoch 129/1000\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 0.2359 - acc: 0.8996\n",
            "Epoch 130/1000\n",
            "60000/60000 [==============================] - 20s 330us/step - loss: 0.2338 - acc: 0.8999\n",
            "Epoch 131/1000\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 0.2363 - acc: 0.8993\n",
            "Epoch 132/1000\n",
            "60000/60000 [==============================] - 20s 330us/step - loss: 0.2349 - acc: 0.8996\n",
            "Epoch 133/1000\n",
            "60000/60000 [==============================] - 20s 329us/step - loss: 0.2348 - acc: 0.8995\n",
            "Epoch 134/1000\n",
            "60000/60000 [==============================] - 20s 330us/step - loss: 0.2372 - acc: 0.8991\n",
            "Epoch 135/1000\n",
            "60000/60000 [==============================] - 20s 329us/step - loss: 0.2354 - acc: 0.8996\n",
            "Epoch 136/1000\n",
            "60000/60000 [==============================] - 20s 333us/step - loss: 0.2350 - acc: 0.8995\n",
            "Epoch 137/1000\n",
            "60000/60000 [==============================] - 20s 328us/step - loss: 0.2347 - acc: 0.8994\n",
            "Epoch 138/1000\n",
            "60000/60000 [==============================] - 20s 330us/step - loss: 0.2355 - acc: 0.8995\n",
            "Epoch 139/1000\n",
            "60000/60000 [==============================] - 20s 330us/step - loss: 0.2345 - acc: 0.8997\n",
            "Epoch 140/1000\n",
            "60000/60000 [==============================] - 20s 332us/step - loss: 0.2327 - acc: 0.9000\n",
            "Epoch 141/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2361 - acc: 0.8993\n",
            "Epoch 142/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2342 - acc: 0.8999\n",
            "Epoch 143/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2342 - acc: 0.9000\n",
            "Epoch 144/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2350 - acc: 0.8997\n",
            "Epoch 145/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2343 - acc: 0.8996\n",
            "Epoch 146/1000\n",
            "60000/60000 [==============================] - 21s 347us/step - loss: 0.2350 - acc: 0.8996\n",
            "Epoch 147/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2353 - acc: 0.8997\n",
            "Epoch 148/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2357 - acc: 0.8997\n",
            "Epoch 149/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2342 - acc: 0.8998\n",
            "Epoch 150/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2334 - acc: 0.9000\n",
            "Epoch 151/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2347 - acc: 0.8997\n",
            "Epoch 152/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2354 - acc: 0.8994\n",
            "Epoch 153/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2350 - acc: 0.8995\n",
            "Epoch 154/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2349 - acc: 0.8998\n",
            "Epoch 155/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2334 - acc: 0.9000\n",
            "Epoch 156/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2342 - acc: 0.9000\n",
            "Epoch 157/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2338 - acc: 0.8997\n",
            "Epoch 158/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2358 - acc: 0.8996\n",
            "Epoch 159/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2344 - acc: 0.8996\n",
            "Epoch 160/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2351 - acc: 0.8995\n",
            "Epoch 161/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2334 - acc: 0.8997\n",
            "Epoch 162/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2352 - acc: 0.8996\n",
            "Epoch 163/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2344 - acc: 0.8996\n",
            "Epoch 164/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2334 - acc: 0.9000\n",
            "Epoch 165/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2347 - acc: 0.9000\n",
            "Epoch 166/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2350 - acc: 0.8997\n",
            "Epoch 167/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2338 - acc: 0.8999\n",
            "Epoch 168/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2341 - acc: 0.9001\n",
            "Epoch 169/1000\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.2326 - acc: 0.9001\n",
            "Epoch 170/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2348 - acc: 0.8998\n",
            "Epoch 171/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2348 - acc: 0.8998\n",
            "Epoch 172/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2347 - acc: 0.8998\n",
            "Epoch 173/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2343 - acc: 0.8999\n",
            "Epoch 174/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2353 - acc: 0.8995\n",
            "Epoch 175/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2339 - acc: 0.8998\n",
            "Epoch 176/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2337 - acc: 0.8997\n",
            "Epoch 177/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2338 - acc: 0.9001\n",
            "Epoch 178/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2344 - acc: 0.8998\n",
            "Epoch 179/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2338 - acc: 0.8999\n",
            "Epoch 180/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2325 - acc: 0.9004\n",
            "Epoch 181/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2355 - acc: 0.8996\n",
            "Epoch 182/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2346 - acc: 0.8999\n",
            "Epoch 183/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2347 - acc: 0.8996\n",
            "Epoch 184/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2333 - acc: 0.9000\n",
            "Epoch 185/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2324 - acc: 0.9003\n",
            "Epoch 186/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2354 - acc: 0.8996\n",
            "Epoch 187/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2343 - acc: 0.9000\n",
            "Epoch 188/1000\n",
            "60000/60000 [==============================] - 20s 342us/step - loss: 0.2352 - acc: 0.8995\n",
            "Epoch 189/1000\n",
            "60000/60000 [==============================] - 21s 348us/step - loss: 0.2346 - acc: 0.8997\n",
            "Epoch 190/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2335 - acc: 0.8997\n",
            "Epoch 191/1000\n",
            "60000/60000 [==============================] - 20s 342us/step - loss: 0.2344 - acc: 0.8996\n",
            "Epoch 192/1000\n",
            "60000/60000 [==============================] - 21s 350us/step - loss: 0.2347 - acc: 0.8999\n",
            "Epoch 193/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2332 - acc: 0.9003\n",
            "Epoch 194/1000\n",
            "60000/60000 [==============================] - 21s 346us/step - loss: 0.2338 - acc: 0.9000\n",
            "Epoch 195/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2328 - acc: 0.9002\n",
            "Epoch 196/1000\n",
            "60000/60000 [==============================] - 21s 350us/step - loss: 0.2352 - acc: 0.8997\n",
            "Epoch 197/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2332 - acc: 0.9001\n",
            "Epoch 198/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2354 - acc: 0.8997\n",
            "Epoch 199/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2341 - acc: 0.9001\n",
            "Epoch 200/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2336 - acc: 0.9000\n",
            "Epoch 201/1000\n",
            "60000/60000 [==============================] - 21s 345us/step - loss: 0.2334 - acc: 0.9001\n",
            "Epoch 202/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2330 - acc: 0.9002\n",
            "Epoch 203/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2341 - acc: 0.8999\n",
            "Epoch 204/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2334 - acc: 0.9002\n",
            "Epoch 205/1000\n",
            "42112/60000 [====================>.........] - ETA: 6s - loss: 0.2329 - acc: 0.9007"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f25b040d304f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m              metrics=['accuracy'])\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9yoh-wsTZHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('weights_204_acc0_90.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_piiq5eTk3r",
        "colab_type": "code",
        "outputId": "d95e6589-7d07-41c1-879a-0074cd67059a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.load_weights('weights_204_acc0_90.h5')\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1000, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.2336 - acc: 0.9001\n",
            "Epoch 2/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2347 - acc: 0.9000\n",
            "Epoch 3/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2342 - acc: 0.8999\n",
            "Epoch 4/1000\n",
            "60000/60000 [==============================] - 21s 346us/step - loss: 0.2321 - acc: 0.9004\n",
            "Epoch 5/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2335 - acc: 0.9000\n",
            "Epoch 6/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2353 - acc: 0.8996\n",
            "Epoch 7/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2319 - acc: 0.9006\n",
            "Epoch 8/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2341 - acc: 0.8999\n",
            "Epoch 9/1000\n",
            "60000/60000 [==============================] - 21s 342us/step - loss: 0.2336 - acc: 0.9000\n",
            "Epoch 10/1000\n",
            "60000/60000 [==============================] - 21s 344us/step - loss: 0.2330 - acc: 0.9002\n",
            "Epoch 11/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2344 - acc: 0.8999\n",
            "Epoch 12/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2339 - acc: 0.9000\n",
            "Epoch 13/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2339 - acc: 0.9000\n",
            "Epoch 14/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2348 - acc: 0.9000\n",
            "Epoch 15/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2324 - acc: 0.9004\n",
            "Epoch 16/1000\n",
            "60000/60000 [==============================] - 21s 349us/step - loss: 0.2339 - acc: 0.9000\n",
            "Epoch 17/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2349 - acc: 0.8998\n",
            "Epoch 18/1000\n",
            "60000/60000 [==============================] - 20s 341us/step - loss: 0.2329 - acc: 0.9001\n",
            "Epoch 19/1000\n",
            "60000/60000 [==============================] - 21s 343us/step - loss: 0.2341 - acc: 0.9000\n",
            "Epoch 20/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2334 - acc: 0.9003\n",
            "Epoch 21/1000\n",
            "60000/60000 [==============================] - 20s 339us/step - loss: 0.2340 - acc: 0.9002\n",
            "Epoch 22/1000\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.2335 - acc: 0.9001\n",
            "Epoch 23/1000\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.2318 - acc: 0.9004\n",
            "Epoch 24/1000\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.2341 - acc: 0.9002\n",
            "Epoch 25/1000\n",
            "60000/60000 [==============================] - 20s 337us/step - loss: 0.2330 - acc: 0.9002\n",
            "Epoch 26/1000\n",
            "60000/60000 [==============================] - 20s 334us/step - loss: 0.2330 - acc: 0.9003\n",
            "Epoch 27/1000\n",
            "60000/60000 [==============================] - 20s 340us/step - loss: 0.2334 - acc: 0.9001\n",
            "Epoch 28/1000\n",
            "33920/60000 [===============>..............] - ETA: 8s - loss: 0.2347 - acc: 0.8997"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5b7d52fdec52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m              metrics=['accuracy'])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6d1Y5kxYCjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('weights_204_acc0_90.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcIcF_bIX_xf",
        "colab_type": "code",
        "outputId": "09f2d6b8-873f-43d1-c8b4-d043f614f6e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import optimizers\n",
        "model.load_weights('weights_204_acc0_90.h5')\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=sgd,\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=30, verbose=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 20s 333us/step - loss: 0.3314 - acc: 0.8814\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2816 - acc: 0.8883\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.2712 - acc: 0.8904\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.2653 - acc: 0.8918\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 0.2642 - acc: 0.8921\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 19s 316us/step - loss: 0.2598 - acc: 0.8930\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2600 - acc: 0.8932\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 19s 317us/step - loss: 0.2569 - acc: 0.8934\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 19s 319us/step - loss: 0.2583 - acc: 0.8932\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.2539 - acc: 0.8946\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 19s 317us/step - loss: 0.2579 - acc: 0.8938\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 19s 317us/step - loss: 0.2525 - acc: 0.8948\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 19s 317us/step - loss: 0.2530 - acc: 0.8953\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 19s 317us/step - loss: 0.2526 - acc: 0.8951\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 19s 317us/step - loss: 0.2536 - acc: 0.8948\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 0.2514 - acc: 0.8953\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 18s 308us/step - loss: 0.2554 - acc: 0.8942\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.2547 - acc: 0.8948\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 18s 306us/step - loss: 0.2497 - acc: 0.8956\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 19s 313us/step - loss: 0.2497 - acc: 0.8959\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 19s 313us/step - loss: 0.2471 - acc: 0.8965\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 18s 306us/step - loss: 0.2517 - acc: 0.8954\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 19s 313us/step - loss: 0.2553 - acc: 0.8948\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 19s 312us/step - loss: 0.2513 - acc: 0.8956\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 18s 307us/step - loss: 0.2489 - acc: 0.8963\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 18s 306us/step - loss: 0.2510 - acc: 0.8954\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 18s 306us/step - loss: 0.2476 - acc: 0.8961\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 18s 306us/step - loss: 0.2494 - acc: 0.8958\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 18s 307us/step - loss: 0.2480 - acc: 0.8960\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.2579 - acc: 0.8940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5ba164dba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH2lX1hgcZ2z",
        "colab_type": "code",
        "outputId": "7e3cbcbc-7c05-423a-d3f2-80714cff0e5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.save_weights('weights_2_acc0_90.h5')\n",
        "from keras import optimizers\n",
        "model.load_weights('weights_2_acc0_90.h5')\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=sgd,\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=30, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 20s 326us/step - loss: 0.2381 - acc: 0.8989\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2330 - acc: 0.9000\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.2332 - acc: 0.9004\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2316 - acc: 0.9005\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.2312 - acc: 0.9007\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 20s 328us/step - loss: 0.2305 - acc: 0.9008\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2302 - acc: 0.9010\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.2302 - acc: 0.9010\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.2293 - acc: 0.9011\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.2292 - acc: 0.9011\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.2295 - acc: 0.9011\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 19s 319us/step - loss: 0.2288 - acc: 0.9014\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.2288 - acc: 0.9013\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.2288 - acc: 0.9013\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.2288 - acc: 0.9013\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2291 - acc: 0.9012\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 19s 319us/step - loss: 0.2287 - acc: 0.9013\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 19s 319us/step - loss: 0.2281 - acc: 0.9016\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 19s 319us/step - loss: 0.2286 - acc: 0.9012\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.2285 - acc: 0.9014\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.2285 - acc: 0.9014\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 20s 326us/step - loss: 0.2281 - acc: 0.9015\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2276 - acc: 0.9016\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.2279 - acc: 0.9018\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.2279 - acc: 0.9016\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.2278 - acc: 0.9017\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.2280 - acc: 0.9015\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.2276 - acc: 0.9016\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 19s 319us/step - loss: 0.2280 - acc: 0.9015\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.2274 - acc: 0.9018\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5ba164dac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bggZaf0gXTn",
        "colab_type": "code",
        "outputId": "b2bc5eef-b9b8-4a47-8ad9-2498921a0bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.save_weights('weights_3_acc0_90.h5')\n",
        "from keras import optimizers\n",
        "model.load_weights('weights_3_acc0_90.h5')\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=sgd,\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=30, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 20s 329us/step - loss: 0.2276 - acc: 0.9015\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 20s 331us/step - loss: 0.2276 - acc: 0.9016\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 20s 330us/step - loss: 0.2273 - acc: 0.9017\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.2272 - acc: 0.9018\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 19s 324us/step - loss: 0.2274 - acc: 0.9017\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.2271 - acc: 0.9018\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 20s 326us/step - loss: 0.2274 - acc: 0.9018\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.2273 - acc: 0.9017\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.2272 - acc: 0.9018\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2278 - acc: 0.9016\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.2271 - acc: 0.9018\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2272 - acc: 0.9019\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.2273 - acc: 0.9018\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2270 - acc: 0.9018\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 20s 325us/step - loss: 0.2274 - acc: 0.9017\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.2273 - acc: 0.9018\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2269 - acc: 0.9019\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 20s 332us/step - loss: 0.2272 - acc: 0.9018\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 0.2272 - acc: 0.9018\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 19s 325us/step - loss: 0.2271 - acc: 0.9019\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.2270 - acc: 0.9019\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 19s 324us/step - loss: 0.2275 - acc: 0.9016\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.2274 - acc: 0.9017\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.2274 - acc: 0.9017\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 20s 329us/step - loss: 0.2275 - acc: 0.9018\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 19s 324us/step - loss: 0.2273 - acc: 0.9018\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.2272 - acc: 0.9018\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 20s 325us/step - loss: 0.2272 - acc: 0.9018\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 19s 325us/step - loss: 0.2270 - acc: 0.9017\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 20s 336us/step - loss: 0.2274 - acc: 0.9016\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5bb1feac88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WOML7gmqw40",
        "colab_type": "code",
        "outputId": "d9d08ac9-7560-4c7c-b1ca-fd2f8f2838f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.save_weights('weights_4_acc0_90.h5')\n",
        "from keras import optimizers\n",
        "model.load_weights('weights_4_acc0_90.h5')\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer=sgd,\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=30, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 19s 319us/step - loss: 0.2269 - acc: 0.9018\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 19s 310us/step - loss: 0.2273 - acc: 0.9018\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 19s 312us/step - loss: 0.2270 - acc: 0.9018\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 19s 312us/step - loss: 0.2268 - acc: 0.9019\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 19s 316us/step - loss: 0.2272 - acc: 0.9018\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.2271 - acc: 0.9017\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.2269 - acc: 0.9018\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.2272 - acc: 0.9018\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.2271 - acc: 0.9019\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.2275 - acc: 0.9018\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.2273 - acc: 0.9018\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.2271 - acc: 0.9018\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.2269 - acc: 0.9018\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.2271 - acc: 0.9018\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.2272 - acc: 0.9018\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.2272 - acc: 0.9018\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.2269 - acc: 0.9019\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.2277 - acc: 0.9016\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.2273 - acc: 0.9017\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.2272 - acc: 0.9018\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.2271 - acc: 0.9018\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.2271 - acc: 0.9018\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.2271 - acc: 0.9017\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.2271 - acc: 0.9018\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.2271 - acc: 0.9018\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.2270 - acc: 0.9018\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.2273 - acc: 0.9018\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.2272 - acc: 0.9018\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.2276 - acc: 0.9018\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 18s 307us/step - loss: 0.2269 - acc: 0.9019\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5ba0ad0e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKtvFQbrOaV7",
        "colab_type": "code",
        "outputId": "91b403ee-5021-4095-d8a8-0313bd5217c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "\n",
        "# from keras.layers import Activation\n",
        "# from keras.models import Sequential\n",
        "# model = Sequential()\n",
        " \n",
        "# model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "# model.add(Convolution2D(10, 1, activation='relu'))\n",
        "# model.add(Convolution2D(10, 26))\n",
        "# model.add(Flatten())\n",
        "# model.add(Activation('softmax'))\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_110 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_111 (Conv2D)          (None, 26, 26, 10)        330       \n",
            "_________________________________________________________________\n",
            "conv2d_112 (Conv2D)          (None, 1, 1, 10)          67610     \n",
            "_________________________________________________________________\n",
            "flatten_20 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 68,260\n",
            "Trainable params: 68,260\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b4gM0vuOcWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOf2-OoSOeLt",
        "colab_type": "code",
        "outputId": "8a6dec91-eccf-4111-ffe2-7d4a6ff6a28d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 19s 325us/step - loss: 0.2068 - acc: 0.9397\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 14s 227us/step - loss: 0.0813 - acc: 0.9752\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 14s 231us/step - loss: 0.0590 - acc: 0.9820\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 14s 231us/step - loss: 0.0467 - acc: 0.9852\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 14s 225us/step - loss: 0.0366 - acc: 0.9882\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 13s 224us/step - loss: 0.0286 - acc: 0.9909\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 13s 223us/step - loss: 0.0244 - acc: 0.9921\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 14s 226us/step - loss: 0.0179 - acc: 0.9944\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 13s 223us/step - loss: 0.0157 - acc: 0.9950\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 13s 224us/step - loss: 0.0121 - acc: 0.9959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa5a7cce6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW5H2-GdVtDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqMf3-NzSit6",
        "colab_type": "code",
        "outputId": "33fce9c3-a48c-4698-e9e2-4215eedb6603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred[:9])\n",
        "print(y_test[:9])\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "layer_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.3211443702697756, 0.0859]\n",
            "[[0.0928592  0.0960931  0.10382872 0.09565974 0.10498116 0.10170414\n",
            "  0.09253392 0.11225241 0.11056384 0.08952373]\n",
            " [0.10105775 0.09820261 0.09634101 0.10257036 0.10445461 0.10440949\n",
            "  0.09472025 0.10405599 0.09845711 0.09573079]\n",
            " [0.09302402 0.09611048 0.09977136 0.10008989 0.10312203 0.09961641\n",
            "  0.09869093 0.10635718 0.10411512 0.09910258]\n",
            " [0.08741726 0.09869978 0.09041867 0.10686553 0.10564237 0.0996066\n",
            "  0.08686195 0.11825424 0.11045969 0.09577385]\n",
            " [0.09576418 0.09840339 0.10134164 0.10170951 0.09111136 0.0979138\n",
            "  0.09798665 0.11075181 0.10424168 0.10077593]\n",
            " [0.09239227 0.09794442 0.10230809 0.1003089  0.10341826 0.09930012\n",
            "  0.09748819 0.10414305 0.10612267 0.09657409]\n",
            " [0.09405836 0.09673701 0.10585125 0.09798262 0.09944199 0.09696466\n",
            "  0.09372951 0.10983273 0.11078255 0.09461939]\n",
            " [0.09656423 0.09616908 0.10178188 0.09394231 0.10289709 0.10704183\n",
            "  0.09879257 0.10468765 0.09877507 0.09934827]\n",
            " [0.10354648 0.09263248 0.09586286 0.09807088 0.10462474 0.09956419\n",
            "  0.09417004 0.10676953 0.10600845 0.09875038]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation_9': <keras.layers.core.Activation at 0x7fa55ad16cf8>,\n",
              " 'conv2d_51': <keras.layers.convolutional.Conv2D at 0x7fa55acf5f60>,\n",
              " 'conv2d_52': <keras.layers.convolutional.Conv2D at 0x7fa55acf5d30>,\n",
              " 'conv2d_53': <keras.layers.convolutional.Conv2D at 0x7fa55acf5da0>,\n",
              " 'flatten_9': <keras.layers.core.Flatten at 0x7fa55acfe7f0>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_aX5Gd0DGbB",
        "colab_type": "code",
        "outputId": "31958018-7f09-4fb4-ea96-9b94ca1b5481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1000, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "60000/60000 [==============================] - 19s 312us/step - loss: 1.1536 - acc: 0.5400\n",
            "Epoch 2/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9682 - acc: 0.5967\n",
            "Epoch 3/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9545 - acc: 0.5996\n",
            "Epoch 4/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9473 - acc: 0.6010\n",
            "Epoch 5/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9427 - acc: 0.6020\n",
            "Epoch 6/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9386 - acc: 0.6029\n",
            "Epoch 7/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9358 - acc: 0.6035\n",
            "Epoch 8/1000\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.9327 - acc: 0.6038\n",
            "Epoch 9/1000\n",
            "60000/60000 [==============================] - 18s 306us/step - loss: 0.9319 - acc: 0.6044\n",
            "Epoch 10/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9297 - acc: 0.6043\n",
            "Epoch 11/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9284 - acc: 0.6047\n",
            "Epoch 12/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9263 - acc: 0.6051\n",
            "Epoch 13/1000\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.9255 - acc: 0.6052\n",
            "Epoch 14/1000\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.9247 - acc: 0.6055\n",
            "Epoch 15/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9224 - acc: 0.6057\n",
            "Epoch 16/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9215 - acc: 0.6059\n",
            "Epoch 17/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9206 - acc: 0.6057\n",
            "Epoch 18/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9198 - acc: 0.6061\n",
            "Epoch 19/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9190 - acc: 0.6062\n",
            "Epoch 20/1000\n",
            "60000/60000 [==============================] - 17s 292us/step - loss: 0.9181 - acc: 0.6063\n",
            "Epoch 21/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9179 - acc: 0.6065\n",
            "Epoch 22/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9169 - acc: 0.6065\n",
            "Epoch 23/1000\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.9159 - acc: 0.6068\n",
            "Epoch 24/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9151 - acc: 0.6069\n",
            "Epoch 25/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9141 - acc: 0.6071\n",
            "Epoch 26/1000\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.9137 - acc: 0.6073\n",
            "Epoch 27/1000\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.9136 - acc: 0.6068\n",
            "Epoch 28/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9127 - acc: 0.6072\n",
            "Epoch 29/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9133 - acc: 0.6072\n",
            "Epoch 30/1000\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.9117 - acc: 0.6073\n",
            "Epoch 31/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9123 - acc: 0.6073\n",
            "Epoch 32/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9105 - acc: 0.6074\n",
            "Epoch 33/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9107 - acc: 0.6076\n",
            "Epoch 34/1000\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.9093 - acc: 0.6078\n",
            "Epoch 35/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9100 - acc: 0.6077\n",
            "Epoch 36/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9096 - acc: 0.6077\n",
            "Epoch 37/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9106 - acc: 0.6074\n",
            "Epoch 38/1000\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.9083 - acc: 0.6079\n",
            "Epoch 39/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9100 - acc: 0.6078\n",
            "Epoch 40/1000\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.9090 - acc: 0.6079\n",
            "Epoch 41/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9081 - acc: 0.6080\n",
            "Epoch 42/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9086 - acc: 0.6079\n",
            "Epoch 43/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9096 - acc: 0.6077\n",
            "Epoch 44/1000\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.9069 - acc: 0.6083\n",
            "Epoch 45/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9076 - acc: 0.6081\n",
            "Epoch 46/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9081 - acc: 0.6080\n",
            "Epoch 47/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9077 - acc: 0.6081\n",
            "Epoch 48/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9068 - acc: 0.6082\n",
            "Epoch 49/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9070 - acc: 0.6082\n",
            "Epoch 50/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9065 - acc: 0.6081\n",
            "Epoch 51/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9074 - acc: 0.6081\n",
            "Epoch 52/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9064 - acc: 0.6082\n",
            "Epoch 53/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9069 - acc: 0.6081\n",
            "Epoch 54/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9056 - acc: 0.6083\n",
            "Epoch 55/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9071 - acc: 0.6080\n",
            "Epoch 56/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9053 - acc: 0.6084\n",
            "Epoch 57/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9072 - acc: 0.6080\n",
            "Epoch 58/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9064 - acc: 0.6082\n",
            "Epoch 59/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9060 - acc: 0.6085\n",
            "Epoch 60/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9048 - acc: 0.6085\n",
            "Epoch 61/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9062 - acc: 0.6081\n",
            "Epoch 62/1000\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.9061 - acc: 0.6081\n",
            "Epoch 63/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9073 - acc: 0.6081\n",
            "Epoch 64/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9061 - acc: 0.6082\n",
            "Epoch 65/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9061 - acc: 0.6082\n",
            "Epoch 66/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9060 - acc: 0.6085\n",
            "Epoch 67/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9053 - acc: 0.6084\n",
            "Epoch 68/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9060 - acc: 0.6083\n",
            "Epoch 69/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9056 - acc: 0.6084\n",
            "Epoch 70/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9064 - acc: 0.6082\n",
            "Epoch 71/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9049 - acc: 0.6085\n",
            "Epoch 72/1000\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.9074 - acc: 0.6081\n",
            "Epoch 73/1000\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.9061 - acc: 0.6082\n",
            "Epoch 74/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9050 - acc: 0.6086\n",
            "Epoch 75/1000\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.9049 - acc: 0.6086\n",
            "Epoch 76/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9056 - acc: 0.6084\n",
            "Epoch 77/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9065 - acc: 0.6081\n",
            "Epoch 78/1000\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.9058 - acc: 0.6085\n",
            "Epoch 79/1000\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.9047 - acc: 0.6086\n",
            "Epoch 80/1000\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.9072 - acc: 0.6082\n",
            "Epoch 81/1000\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.9047 - acc: 0.6085\n",
            "Epoch 82/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9057 - acc: 0.6083\n",
            "Epoch 83/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9049 - acc: 0.6085\n",
            "Epoch 84/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9047 - acc: 0.6085\n",
            "Epoch 85/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9044 - acc: 0.6087\n",
            "Epoch 86/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9056 - acc: 0.6085\n",
            "Epoch 87/1000\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.9056 - acc: 0.6085\n",
            "Epoch 88/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9067 - acc: 0.6084\n",
            "Epoch 89/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9054 - acc: 0.6084\n",
            "Epoch 90/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9052 - acc: 0.6088\n",
            "Epoch 91/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9052 - acc: 0.6086\n",
            "Epoch 92/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9049 - acc: 0.6087\n",
            "Epoch 93/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9066 - acc: 0.6082\n",
            "Epoch 94/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9040 - acc: 0.6088\n",
            "Epoch 95/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9067 - acc: 0.6083\n",
            "Epoch 96/1000\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.9044 - acc: 0.6087\n",
            "Epoch 97/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9048 - acc: 0.6087\n",
            "Epoch 98/1000\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.9067 - acc: 0.6083\n",
            "Epoch 99/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9045 - acc: 0.6086\n",
            "Epoch 100/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9040 - acc: 0.6089\n",
            "Epoch 101/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9040 - acc: 0.6088\n",
            "Epoch 102/1000\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.9065 - acc: 0.6082\n",
            "Epoch 103/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9046 - acc: 0.6088\n",
            "Epoch 104/1000\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.9053 - acc: 0.6085\n",
            "Epoch 105/1000\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.9041 - acc: 0.6088\n",
            "Epoch 106/1000\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.9055 - acc: 0.6083\n",
            "Epoch 107/1000\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.9059 - acc: 0.6085\n",
            "Epoch 108/1000\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.9055 - acc: 0.6086\n",
            "Epoch 109/1000\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.9061 - acc: 0.6087\n",
            "Epoch 110/1000\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.9046 - acc: 0.6087\n",
            "Epoch 111/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9065 - acc: 0.6083\n",
            "Epoch 112/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9045 - acc: 0.6087\n",
            "Epoch 113/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9059 - acc: 0.6086\n",
            "Epoch 114/1000\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.9059 - acc: 0.6086\n",
            "Epoch 115/1000\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.9042 - acc: 0.6087\n",
            "Epoch 116/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9043 - acc: 0.6085\n",
            "Epoch 117/1000\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.9049 - acc: 0.6084\n",
            "Epoch 118/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9045 - acc: 0.6088\n",
            "Epoch 119/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9059 - acc: 0.6082\n",
            "Epoch 120/1000\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.9039 - acc: 0.6088\n",
            "Epoch 121/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9050 - acc: 0.6085\n",
            "Epoch 122/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9049 - acc: 0.6086\n",
            "Epoch 123/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9051 - acc: 0.6086\n",
            "Epoch 124/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9049 - acc: 0.6086\n",
            "Epoch 125/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9057 - acc: 0.6085\n",
            "Epoch 126/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9050 - acc: 0.6086\n",
            "Epoch 127/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9048 - acc: 0.6086\n",
            "Epoch 128/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9063 - acc: 0.6083\n",
            "Epoch 129/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9046 - acc: 0.6086\n",
            "Epoch 130/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9058 - acc: 0.6084\n",
            "Epoch 131/1000\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.9046 - acc: 0.6084\n",
            "Epoch 132/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9070 - acc: 0.6082\n",
            "Epoch 133/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9041 - acc: 0.6090\n",
            "Epoch 134/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9045 - acc: 0.6086\n",
            "Epoch 135/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9036 - acc: 0.6088\n",
            "Epoch 136/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9067 - acc: 0.6081\n",
            "Epoch 137/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9049 - acc: 0.6087\n",
            "Epoch 138/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9072 - acc: 0.6081\n",
            "Epoch 139/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9073 - acc: 0.6084\n",
            "Epoch 140/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9044 - acc: 0.6088\n",
            "Epoch 141/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9046 - acc: 0.6088\n",
            "Epoch 142/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9058 - acc: 0.6085\n",
            "Epoch 143/1000\n",
            "60000/60000 [==============================] - 18s 297us/step - loss: 0.9068 - acc: 0.6083\n",
            "Epoch 144/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9061 - acc: 0.6083\n",
            "Epoch 145/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9068 - acc: 0.6083\n",
            "Epoch 146/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9063 - acc: 0.6085\n",
            "Epoch 147/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9058 - acc: 0.6086\n",
            "Epoch 148/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9032 - acc: 0.6089\n",
            "Epoch 149/1000\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.9049 - acc: 0.6086\n",
            "Epoch 150/1000\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.9057 - acc: 0.6083\n",
            "Epoch 151/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9059 - acc: 0.6087\n",
            "Epoch 152/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9055 - acc: 0.6085\n",
            "Epoch 153/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9062 - acc: 0.6085\n",
            "Epoch 154/1000\n",
            "60000/60000 [==============================] - 18s 299us/step - loss: 0.9035 - acc: 0.6089\n",
            "Epoch 155/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9055 - acc: 0.6084\n",
            "Epoch 156/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9060 - acc: 0.6085\n",
            "Epoch 157/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9054 - acc: 0.6086\n",
            "Epoch 158/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9050 - acc: 0.6087\n",
            "Epoch 159/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9073 - acc: 0.6081\n",
            "Epoch 160/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9066 - acc: 0.6084\n",
            "Epoch 161/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9095 - acc: 0.6079\n",
            "Epoch 162/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9055 - acc: 0.6086\n",
            "Epoch 163/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9050 - acc: 0.6085\n",
            "Epoch 164/1000\n",
            "60000/60000 [==============================] - 18s 298us/step - loss: 0.9056 - acc: 0.6086\n",
            "Epoch 165/1000\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.9055 - acc: 0.6083\n",
            "Epoch 166/1000\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.9066 - acc: 0.6083\n",
            "Epoch 167/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9082 - acc: 0.6081\n",
            "Epoch 168/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9071 - acc: 0.6082\n",
            "Epoch 169/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9094 - acc: 0.6078\n",
            "Epoch 170/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9074 - acc: 0.6081\n",
            "Epoch 171/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9063 - acc: 0.6086\n",
            "Epoch 172/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9082 - acc: 0.6083\n",
            "Epoch 173/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9074 - acc: 0.6083\n",
            "Epoch 174/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9081 - acc: 0.6080\n",
            "Epoch 175/1000\n",
            "60000/60000 [==============================] - 17s 290us/step - loss: 0.9059 - acc: 0.6086\n",
            "Epoch 176/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9057 - acc: 0.6086\n",
            "Epoch 177/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9085 - acc: 0.6082\n",
            "Epoch 178/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9045 - acc: 0.6090\n",
            "Epoch 179/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9063 - acc: 0.6084\n",
            "Epoch 180/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9070 - acc: 0.6083\n",
            "Epoch 181/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9078 - acc: 0.6084\n",
            "Epoch 182/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9055 - acc: 0.6085\n",
            "Epoch 183/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9067 - acc: 0.6085\n",
            "Epoch 184/1000\n",
            "60000/60000 [==============================] - 18s 301us/step - loss: 0.9086 - acc: 0.6080\n",
            "Epoch 185/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9060 - acc: 0.6088\n",
            "Epoch 186/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9060 - acc: 0.6087\n",
            "Epoch 187/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9070 - acc: 0.6081\n",
            "Epoch 188/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9081 - acc: 0.6085\n",
            "Epoch 189/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9064 - acc: 0.6084\n",
            "Epoch 190/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9114 - acc: 0.6077\n",
            "Epoch 191/1000\n",
            "60000/60000 [==============================] - 17s 292us/step - loss: 0.9062 - acc: 0.6085\n",
            "Epoch 192/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9072 - acc: 0.6083\n",
            "Epoch 193/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9084 - acc: 0.6081\n",
            "Epoch 194/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9086 - acc: 0.6081\n",
            "Epoch 195/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9047 - acc: 0.6086\n",
            "Epoch 196/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9062 - acc: 0.6083\n",
            "Epoch 197/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9050 - acc: 0.6084\n",
            "Epoch 198/1000\n",
            "60000/60000 [==============================] - 18s 295us/step - loss: 0.9049 - acc: 0.6084\n",
            "Epoch 199/1000\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.9085 - acc: 0.6080\n",
            "Epoch 200/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9096 - acc: 0.6081\n",
            "Epoch 201/1000\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.9073 - acc: 0.6083\n",
            "Epoch 202/1000\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.9103 - acc: 0.6079\n",
            "Epoch 203/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9087 - acc: 0.6080\n",
            "Epoch 204/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9060 - acc: 0.6084\n",
            "Epoch 205/1000\n",
            "60000/60000 [==============================] - 18s 294us/step - loss: 0.9116 - acc: 0.6075\n",
            "Epoch 206/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9085 - acc: 0.6081\n",
            "Epoch 207/1000\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.9064 - acc: 0.6081\n",
            "Epoch 208/1000\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9083 - acc: 0.6081\n",
            "Epoch 209/1000\n",
            "60000/60000 [==============================] - 18s 296us/step - loss: 0.9070 - acc: 0.6080\n",
            "Epoch 210/1000\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.9077 - acc: 0.6084\n",
            "Epoch 211/1000\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.9094 - acc: 0.6076\n",
            "Epoch 212/1000\n",
            "60000/60000 [==============================] - 18s 304us/step - loss: 0.9065 - acc: 0.6083\n",
            "Epoch 213/1000\n",
            "60000/60000 [==============================] - 18s 300us/step - loss: 0.9104 - acc: 0.6081\n",
            "Epoch 214/1000\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.9076 - acc: 0.6078\n",
            "Epoch 215/1000\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.9080 - acc: 0.6079\n",
            "Epoch 216/1000\n",
            "22144/60000 [==========>...................] - ETA: 11s - loss: 0.9163 - acc: 0.6042"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-ce7bfdef7ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m              metrics=['accuracy'])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylIxey-bK5GE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('weights215_acc60')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkq7k95QLqMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P99DWiwtLs1B",
        "colab_type": "code",
        "outputId": "5e1b4e65-ad05-4ac9-a272-be1ad41d86b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_GSyCIBLybr",
        "colab_type": "code",
        "outputId": "0cafb100-77db-41ac-9695-d87bab8d7727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  weights215_acc60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1yVWqxGUJKw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtY0pFrgUJ_7",
        "colab_type": "code",
        "outputId": "5673dffe-7c2b-4151-f76e-23201754eee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrThabWPUttS",
        "colab_type": "code",
        "outputId": "5ea1de5e-9dc0-4366-ca89-a7a4620aaf93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A67xMt2kUvrA",
        "colab_type": "code",
        "outputId": "7dfc1ab8-b053-429f-b269-e5d6e79a8aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data  weights_204_acc0_90  weights_204_acc0_90.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If0SDSEpU2IA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp weights_204_acc0_90.h5  /content/drive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ypn8j2OL1fg",
        "colab_type": "code",
        "outputId": "9e101dd2-4fdb-4069-ee45-efd31d7f251f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cp weights215_acc60.h5 '/content/drive/My Drive'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'weights215_acc60.h5': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWR9VTSMQSqV",
        "colab_type": "code",
        "outputId": "9e34c527-2049-427a-fd9e-916edb9f75f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_14'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-19229f66b51a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filter %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mplot_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mvis_img_in_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-19229f66b51a>\u001b[0m in \u001b[0;36mvis_img_in_filter\u001b[0;34m(img, layer_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n\u001b[1;32m     23\u001b[0m                       layer_name = 'conv2d_14'):\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mimg_ascs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilter_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'conv2d_14'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54Om8OpAOgIg",
        "colab_type": "code",
        "outputId": "7aeb9612-7712-4bd5-902d-ba2b031882cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "import keras\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), input_shape=(28, 28, 1)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(24,(3,3), activation='relu' ))\n",
        "model.add(MaxPooling2D(pool_size = (2,2), strides= 1, padding='same'))\n",
        "\n",
        "model.add(Conv2D(18, (1,1), activation='relu'))\n",
        "model.add(Conv2D(16,(1,3), activation='relu' ))\n",
        "model.add(Conv2D(14,(3,1), activation='relu' ))\n",
        "model.add(MaxPooling2D(pool_size = (2,2), strides= 2,  padding='same'))\n",
        "\n",
        "model.add(Conv2D(18, (1,1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2), strides= 1,  padding='same'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(12,(3,3), activation='relu' ))\n",
        "model.add(Dropout(0.33))\n",
        "\n",
        "model.add(Convolution2D(10, 9))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0aa00e06bcee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Conv2D' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdZ6nV3XT08l",
        "colab_type": "code",
        "outputId": "35daec29-a504-422c-eb32-2db0c72070ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D\n",
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
        "model.add(Convolution2D(10, 1, activation='relu'))\n",
        "model.add(Convolution2D(10, 24))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_47 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 24, 24, 32)        4640      \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 24, 24, 10)        330       \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 1, 1, 10)          57610     \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 62,740\n",
            "Trainable params: 62,740\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8kWyI9QUSdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}